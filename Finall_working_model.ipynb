{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM1dFhJ-L-Ul"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErZMb2misxNB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7mTtshfO2Jn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWaIEcPvMBYt"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ARcZc4HMFdv"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets list -s \"brain tumor\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32J1N7BKMWJg"
      },
      "outputs": [],
      "source": [
        "\n",
        "!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUSE-We5MoOJ"
      },
      "outputs": [],
      "source": [
        "!ls /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCrsXIrGM5w4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "for split in [\"Training\", \"Testing\"]:\n",
        "    split_path = os.path.join(\"/content\", split)\n",
        "    print(f\"\\n{split.upper()} DATA\")\n",
        "    for cls in os.listdir(split_path):\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        print(f\"{cls:15} -> {len(os.listdir(cls_path))} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIefMUZ6M_0b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Parameters\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Train transforms (with augmentation)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Test transforms (no augmentation)\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = datasets.ImageFolder(\"/content/Training\", transform=train_transforms)\n",
        "test_dataset  = datasets.ImageFolder(\"/content/Testing\", transform=test_transforms)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Classes\n",
        "class_names = train_dataset.classes\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Total training images:\", len(train_dataset))\n",
        "print(\"Total testing images:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_image = dncnn(noisy_image)\n",
        "prediction = vit(clean_image)\n"
      ],
      "metadata": {
        "id": "6NhezXfxIFau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQxEEG2SNIqp"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6FInh89NLJs"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "# Device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Load pretrained ViT model\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=len(class_names))\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 5  # keep small first\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss, correct = 0, 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "    train_acc = correct / len(train_dataset)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFR4czDdTX6Y"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ6CMPJjq8Gq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUdiwaZVTkHG"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# âœ… Run evaluation\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "print(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"âœ… Test Loss: {test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMq-hqEdVyNj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_DsVM3_T5p9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collect predictions & true labels\n",
        "y_true, y_pred = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Classification Report\n",
        "classes = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "print(\"ðŸ”Ž Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwYSDNOvVzSe"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"brain_tumor_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enxsvmSOWQWR"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"brain_tumor_model.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnl5dyHaWvN2"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"brain_tumor_model.pth\"))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ct3fdGPW_xY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from timm import create_model  # ðŸ‘ˆ use timm since you trained a ViT\n",
        "\n",
        "# Load model (recreate architecture)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"brain_tumor_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define transforms (same as training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])   # use same normalization you used while training\n",
        "])\n",
        "\n",
        "# Prediction function\n",
        "def predict(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "    classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no tumor\"]\n",
        "    return classes[pred.item()]\n",
        "\n",
        "# Test prediction\n",
        "print(\"Prediction:\", predict(\"sample_image1.jpg\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z55AwP1jX7P3"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Preprocessing\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Correct path âœ…\n",
        "test_dir = \"Testing\"\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Classes:\", test_dataset.classes)\n",
        "print(\"Total test images:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MwRDJRYYPTd"
      },
      "outputs": [],
      "source": [
        "# âœ… Evaluate on test set\n",
        "test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
        "print(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"âœ… Test Loss: {test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcS20gKzY_6J"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp brain_tumor_model.pth /content/drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzLontozZiOr"
      },
      "outputs": [],
      "source": [
        "# Copy the trained model to your Google Drive\n",
        "!cp brain_tumor_model.pth /content/drive/MyDrive/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l9QIbmUaaXj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from timm import create_model  # ðŸ‘ˆ use timm since you trained a ViT\n",
        "\n",
        "# Load model (recreate architecture)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"brain_tumor_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define transforms (same as training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])   # use same normalization you used while training\n",
        "])\n",
        "\n",
        "# Prediction function\n",
        "def predict(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "    classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no tumor\"]\n",
        "    return classes[pred.item()]\n",
        "\n",
        "# Test prediction\n",
        "print(\"Prediction:\", predict(\"sample_image3.jpg\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgpbPlXZgZxa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFXa6z0xgzQP"
      },
      "outputs": [],
      "source": [
        "!cd /content/drive/MyDrive/CollabNotebooks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swlJ6Ge9mFfU"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/CollabNotebooks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVdXIaH9nOH8"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "print(glob.glob(\"/content/**/brain_tumor_model.pth\", recursive=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xf4BeRhfFU8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)\n",
        "\n",
        "# Use the correct path\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location=device))\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucB4ysTcjy2N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-RI5lQkj5P1"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/ColabNotebooks/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbkjRTbQkOVr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from timm import create_model\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Recreate the model architecture\n",
        "model = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)\n",
        "\n",
        "# Load trained weights from your saved file\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location=device))\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Same transforms you used for training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])   # adjust if you used ImageNet normalization\n",
        "])\n",
        "\n",
        "# Prediction function\n",
        "def predict(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "    classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no tumor\"]\n",
        "    return classes[pred.item()]\n",
        "\n",
        "# Example test\n",
        "print(\"Prediction:\", predict(\"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIqSpnF9k8Zc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_yBenrUe6W4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from timm import create_model  # ðŸ‘ˆ use timm since you trained a ViT\n",
        "\n",
        "# Load model (recreate architecture)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define transforms (same as training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])   # use same normalization you used while training\n",
        "])\n",
        "\n",
        "# Prediction function\n",
        "def predict(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "    classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no tumor\"]\n",
        "    return classes[pred.item()]\n",
        "\n",
        "# Test prediction\n",
        "print(\"Prediction:\", predict(\"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75J01t4viZ6J"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D60P0cJ2irDb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from timm import create_model\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model\n",
        "model = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Prediction function\n",
        "def predict(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "    return pred.item()\n",
        "\n",
        "# Load test CSV\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/test_labels.csv\")\n",
        "\n",
        "# Classes in order\n",
        "classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no tumor\"]\n",
        "\n",
        "correct = 0\n",
        "total = len(test_df)\n",
        "\n",
        "for index, row in tqdm(test_df.iterrows(), total=total):\n",
        "    image_path = row['image_name']\n",
        "    true_label = row['label']\n",
        "\n",
        "    # Make sure image path includes folder if needed\n",
        "    full_image_path = f\"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"\n",
        "\n",
        "    pred_index = predict(full_image_path)\n",
        "    pred_label = classes[pred_index]\n",
        "\n",
        "    if pred_label == true_label:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = (correct / total) * 100\n",
        "print(f\"\\nâœ… Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW9WHVnPk71Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from timm import create_model\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model\n",
        "model = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Prediction function\n",
        "def predict(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "    return pred.item()\n",
        "\n",
        "# Load test_labels.csv\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/test_labels.csv\")\n",
        "\n",
        "# Class names\n",
        "classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no tumor\"]\n",
        "\n",
        "correct = 0\n",
        "total = len(test_df)\n",
        "\n",
        "for index, row in tqdm(test_df.iterrows(), total=total):\n",
        "    image_name = row['image_name']  # e.g., sample-img1-meningioma.jpg\n",
        "    true_label = row['label']\n",
        "\n",
        "    full_image_path = f\"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"\n",
        "\n",
        "    pred_index = predict(full_image_path)\n",
        "    pred_label = classes[pred_index]\n",
        "\n",
        "    if pred_label == true_label:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = (correct / total) * 100\n",
        "print(f\"\\nâœ… Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp0pPpQIn6VT"
      },
      "outputs": [],
      "source": [
        "for index, row in test_df.iterrows():\n",
        "    image_name = row['image_name']\n",
        "    true_label = row['label']\n",
        "    full_image_path = f\"/content/drive/MyDrive/test_images/testimages_files\"\n",
        "\n",
        "    pred_index = predict(full_image_path)\n",
        "    pred_label = classes[pred_index]\n",
        "\n",
        "    print(f\"Image: {image_name}, True Label: {true_label}, Predicted Label: {pred_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FxJmFNyo0Xq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "class TumorDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no tumor\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx, 0]\n",
        "        label_name = self.data.iloc[idx, 1]\n",
        "        label = self.classes.index(label_name)\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zuiCpazo3VK"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "dataset = TumorDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/train_labels.csv\",\n",
        "    img_dir=\"/content/drive/MyDrive/train_images/\",\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Apply val_transform to validation dataset\n",
        "val_dataset.dataset.transform = val_transform\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb0fGU6-qQQ0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_AWmhZkqYIb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create the Kaggle directory\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Move the uploaded kaggle.json to the correct directory\n",
        "shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "\n",
        "# Set permissions for the file\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXBRgxBHqcaW"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset -p /content/drive/MyDrive/brain_tumor_mri_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUMdjcDJqghw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def create_labels_csv(data_dir, output_csv):\n",
        "    data = []\n",
        "    for label in os.listdir(data_dir):\n",
        "        label_dir = os.path.join(data_dir, label)\n",
        "        if os.path.isdir(label_dir):\n",
        "            for img_file in os.listdir(label_dir):\n",
        "                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    data.append({\n",
        "                        \"image_name\": img_file,\n",
        "                        \"label\": label.replace('_', ' ')  # keep label consistent (optional)\n",
        "                    })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"âœ… CSV saved to: {output_csv}\")\n",
        "\n",
        "# Generate train_labels.csv\n",
        "create_labels_csv(\n",
        "    data_dir=\"/content/drive/MyDrive/brain_tumor_mri_dataset/Training\",\n",
        "    output_csv=\"/content/drive/MyDrive/train_labels.csv\"\n",
        ")\n",
        "\n",
        "# Generate test_labels.csv\n",
        "create_labels_csv(\n",
        "    data_dir=\"/content/drive/MyDrive/brain_tumor_mri_dataset/Testing\",\n",
        "    output_csv=\"/content/drive/MyDrive/test_labels.csv\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNN6iLSzqwp9"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/train_labels.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/test_labels.csv\")\n",
        "\n",
        "print(train_df.head())\n",
        "print(test_df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2ME5CZpsCTc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk7ryxoksIDt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- Step 1: Paths ---\n",
        "train_path = '/content/drive/MyDrive/brain_tumor_mri_dataset/Training'\n",
        "test_path = '/content/drive/MyDrive/brain_tumor_mri_dataset/Testing'\n",
        "\n",
        "# --- Step 2: Data Augmentation ---\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# --- Step 3: Load Pretrained Model ---\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze base layers\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# --- Step 4: Compile ---\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# --- Step 5: Train (Initial) ---\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=test_generator\n",
        ")\n",
        "\n",
        "# --- Step 6: Fine-tune (Optional for >90% accuracy) ---\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False  # freeze first layers, fine-tune last 20\n",
        "\n",
        "model.compile(optimizer=Adam(1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=test_generator\n",
        ")\n",
        "\n",
        "# --- Step 7: Evaluate ---\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "print(\"Test Accuracy:\", acc*100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJa3B46V2xSw"
      },
      "outputs": [],
      "source": [
        "print(train_generator.class_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuswjttA3N4i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# ---------------- Paths ----------------\n",
        "train_path = '/content/drive/MyDrive/brain_tumor_mri_dataset/Training'\n",
        "test_path = '/content/drive/MyDrive/brain_tumor_mri_dataset/Testing'\n",
        "\n",
        "# ---------------- Data Augmentation ----------------\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# ---------------- Class Weights ----------------\n",
        "classes = train_generator.classes\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(classes),\n",
        "    y=classes\n",
        ")\n",
        "class_weights = dict(enumerate(weights))\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# ---------------- Load Pretrained ViT ----------------\n",
        "base_model = tf.keras.applications.ViT(\n",
        "    include_top=False,\n",
        "    pretrained='imagenet21k',  # pre-trained on ImageNet21k\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "base_model.trainable = False  # freeze base\n",
        "\n",
        "# ---------------- Add Classification Head ----------------\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "outputs = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# ---------------- Compile ----------------\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ---------------- Callbacks ----------------\n",
        "checkpoint = ModelCheckpoint('best_vit_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "earlystop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# ---------------- Train (Initial) ----------------\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=test_generator,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, earlystop]\n",
        ")\n",
        "\n",
        "# ---------------- Fine-tune ----------------\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-10]:  # freeze all except last 10 transformer blocks\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=test_generator,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, earlystop]\n",
        ")\n",
        "\n",
        "# ---------------- Evaluate ----------------\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "print(\"Test Accuracy:\", acc*100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6T_lYOU3xGU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Paths\n",
        "train_path = '/content/drive/MyDrive/brain_tumor_mri_dataset/Training'\n",
        "test_path = '/content/drive/MyDrive/brain_tumor_mri_dataset/Testing'\n",
        "\n",
        "print(\"Step 1: Libraries imported and paths set.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9NQ8s7u32ZI"
      },
      "outputs": [],
      "source": [
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Only rescaling for test\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Step 2: Data generators ready.\")\n",
        "print(\"Classes:\", train_generator.class_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aijCccqX3-0s"
      },
      "outputs": [],
      "source": [
        "# ---------------- Step 3: Class Weights ----------------\n",
        "classes = train_generator.classes\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(classes),\n",
        "    y=classes\n",
        ")\n",
        "class_weights = dict(enumerate(weights))\n",
        "print(\"Step 3: Class weights computed:\", class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrCizLwe4lSv"
      },
      "outputs": [],
      "source": [
        "from transformers import TFViTForImageClassification, ViTFeatureExtractor\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "# Load model and convert from PyTorch weights\n",
        "model = TFViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_classes,\n",
        "    from_pt=True  # <-- convert PyTorch weights to TensorFlow\n",
        ")\n",
        "\n",
        "print(\"ViT loaded in TensorFlow with classification head for 4 classes.\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B56TGWYT4u5R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFViTForImageClassification, ViTFeatureExtractor\n",
        "\n",
        "# Load pre-trained ViT\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "vit_model = TFViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_classes,\n",
        "    from_pt=True\n",
        ")\n",
        "\n",
        "# Wrap HuggingFace model in a Keras Model\n",
        "class ViTClassifier(tf.keras.Model):\n",
        "    def __init__(self, vit_model):\n",
        "        super().__init__()\n",
        "        self.vit = vit_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # HuggingFace expects \"pixel_values\" key\n",
        "        outputs = self.vit(pixel_values=inputs)\n",
        "        return outputs.logits\n",
        "\n",
        "model = ViTClassifier(vit_model)\n",
        "\n",
        "# Compile like normal Keras model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Wrapped HuggingFace ViT ready for standard Keras training with callbacks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcZqa50t5Onj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlAMS7At5ZoV"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/brain_tumor_mri_dataset/Training'\n",
        "test_path  = '/content/drive/MyDrive/brain_tumor_mri_dataset/Testing'\n",
        "\n",
        "classes = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "num_classes = len(classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArcrxfnZ5zQH"
      },
      "outputs": [],
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean,\n",
        "                         std=feature_extractor.image_std)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean,\n",
        "                         std=feature_extractor.image_std)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb5Q_nGT52Ci"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.ImageFolder(train_path, transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(test_path, transform=test_transforms)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRusHna_56Wp"
      },
      "outputs": [],
      "source": [
        "labels = [label for _, label in train_dataset.imgs]\n",
        "class_weights_values = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights_values, dtype=torch.float32)\n",
        "\n",
        "print(\"Class weights:\", class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEbcJnhc59xa"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=num_classes\n",
        ")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyIYzeOr6A1V"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA6TVWhV6Fb9"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f} - Accuracy: {train_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVoqZCIT-rw3"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_acc = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "222S1ovY-zFa"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/brain_tumor_vit_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSkRxtsQ_C4K"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpdEIEe4_Em7"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(model_path)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrkAe87V_KDK"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"  # replace with your image\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Preprocess\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgD-pqUf_S7H"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class_idx = logits.argmax(-1).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7sNkP1G_VNB"
      },
      "outputs": [],
      "source": [
        "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "predicted_label = class_names[predicted_class_idx]\n",
        "\n",
        "print(f\"Predicted Tumor Type: {predicted_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwkflA8l_kaZ"
      },
      "outputs": [],
      "source": [
        "# ---------------- Single Image Prediction Script ----------------\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "\n",
        "# ---------------- Step 1: Load Trained Model ----------------\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"  # Path to your trained ViT\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(model_path)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# ---------------- Step 2: Define Class Names ----------------\n",
        "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# ---------------- Step 3: Prediction Function ----------------\n",
        "def predict_tumor(image_path):\n",
        "    # Load and convert image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Preprocess image\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class_idx = logits.argmax(-1).item()\n",
        "\n",
        "    # Map index to label\n",
        "    predicted_label = class_names[predicted_class_idx]\n",
        "    return predicted_label\n",
        "\n",
        "# ---------------- Step 4: Test with Any Image ----------------\n",
        "image_path = \"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"  # change this path\n",
        "predicted_tumor = predict_tumor(image_path)\n",
        "print(f\"Predicted Tumor Type: {predicted_tumor}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi-qxPv7ZOzK"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /tmp/brain_tumor_vit_model\n",
        "!cp -r /content/drive/MyDrive/brain_tumor_vit_model/* /tmp/brain_tumor_vit_model/\n",
        "print(\"Step 1 âœ…: Model copied locally.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uiL6PXJIiSo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\"/tmp/brain_tumor_vit_model\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "print(\"Step 2 âœ…: Model and feature extractor loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdShw4rGIkXz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/test_labels.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"Step 3 âœ…: CSV loaded with {len(df)} samples.\")\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1StDzsiItyY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "def predict_tumor(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predicted_class_idx = outputs.logits.argmax(-1).item()\n",
        "\n",
        "    return class_names[predicted_class_idx]\n",
        "\n",
        "print(\"Step 4 âœ…: Prediction function ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FgimaNkNdwx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/precision_check.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"âœ… Loaded {len(df)} rows from CSV.\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPMC4PUvNjz9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import os\n",
        "\n",
        "# Step 1: Copy model locally\n",
        "!mkdir -p /tmp/brain_tumor_vit_model\n",
        "!cp -r ///* /tmp/brain_tumor_vit_model/\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ViTForImageClassification.from_pretrained(\"/tmp/brain_tumor_vit_model\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Step 2: Load CSV file\n",
        "csv_path = '/content/drive/MyDrive/precision_check.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"âœ… Loaded {len(df)} test samples from precision_check.csv\")\n",
        "\n",
        "# Step 3: Prediction function\n",
        "def predict_tumor(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predicted_class_idx = outputs.logits.argmax(-1).item()\n",
        "\n",
        "    return class_names[predicted_class_idx]\n",
        "\n",
        "# Step 4: Predict and Compute Accuracy\n",
        "results = []\n",
        "correct = 0\n",
        "total = len(df)\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    img_path = os.path.join('/content/drive/MyDrive/test_images', row['image_name'])\n",
        "    print(f\"Predicting {row['image_name']} at {img_path} ...\")\n",
        "\n",
        "    pred_label = predict_tumor(img_path)\n",
        "    is_correct = (pred_label == row['label'])\n",
        "\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "\n",
        "    results.append({\n",
        "        'image_name': row['image_name'],\n",
        "        'true_label': row['label'],\n",
        "        'predicted_label': pred_label,\n",
        "        'correct': is_correct\n",
        "    })\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"\\nâœ… Final Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Step 5: Save Results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('/content/prediction_results.csv', index=False)\n",
        "print(\"\\nâœ… Predictions saved to prediction_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWf2QlGjSqNH"
      },
      "outputs": [],
      "source": [
        "!pip install gradio transformers torch torchvision\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG5SQiTsV9Ra"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load model from Drive backup\n",
        "!mkdir -p /tmp/brain_tumor_vit_model\n",
        "!cp -r /content/drive/MyDrive/brain_tumor_vit_model_backup/* /tmp/brain_tumor_vit_model/\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\"/tmp/brain_tumor_vit_model\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Processor\n",
        "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# Class names\n",
        "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNPf5s1OWDu5"
      },
      "outputs": [],
      "source": [
        "def predict_tumor(image):\n",
        "    \"\"\"\n",
        "    Input: PIL Image\n",
        "    Output: Dictionary with tumor prediction and confidence\n",
        "    \"\"\"\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_idx = logits.argmax(-1).item()\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Return class and confidence\n",
        "    return {class_names[i]: float(probs[0][i]) for i in range(len(class_names))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FaWKlY7qWI1h"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=predict_tumor,                   # Function to run\n",
        "    inputs=gr.Image(type=\"pil\"),        # Upload image\n",
        "    outputs=gr.Label(num_top_classes=4), # Show top 4 class probabilities\n",
        "    title=\"Brain Tumor Detection\",\n",
        "    description=\"Upload an MRI image and get the predicted tumor type.\",\n",
        "    examples=[[\"/content/drive/MyDrive/test_images/sample-img1-meningioma.jpg\"],\n",
        "              [\"/content/drive/MyDrive/test_images/sample-img3-glioma.jpg\"]]\n",
        ")\n",
        "\n",
        "ui.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sXHrCNfJyJX"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29P9r_IVK3Nu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import os\n",
        "\n",
        "# Step 1: Copy model locally\n",
        "!mkdir -p /tmp/brain_tumor_vit_model\n",
        "!cp -r /content/drive/MyDrive/brain_tumor_vit_model_backup//* /tmp/brain_tumor_vit_model/\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ViTForImageClassification.from_pretrained(\"/tmp/brain_tumor_vit_model\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# Step 2: Load CSV file\n",
        "csv_path = '/content/drive/MyDrive/precision_check.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"âœ… Loaded {len(df)} test samples from precision_check.csv\")\n",
        "\n",
        "# Step 3: Prediction function\n",
        "def predict_tumor(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predicted_class_idx = outputs.logits.argmax(-1).item()\n",
        "\n",
        "    return class_names[predicted_class_idx]\n",
        "\n",
        "# Step 4: Predict and Compute Accuracy\n",
        "results = []\n",
        "correct = 0\n",
        "total = len(df)\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    img_path = os.path.join('/content/drive/MyDrive/test_images', row['image_name'])\n",
        "    print(f\"Predicting {row['image_name']} at {img_path} ...\")\n",
        "\n",
        "    pred_label = predict_tumor(img_path)\n",
        "    is_correct = (pred_label == row['label'])\n",
        "\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "\n",
        "    results.append({\n",
        "        'image_name': row['image_name'],\n",
        "        'true_label': row['label'],\n",
        "        'predicted_label': pred_label,\n",
        "        'correct': is_correct\n",
        "    })\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"\\nâœ… Final Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Step 5: Save Results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('/content/prediction_results.csv', index=False)\n",
        "print(\"\\nâœ… Predictions saved to prediction_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1JCZn-8FADG"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "base_model = \"google/vit-base-patch16-224\"\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(base_model)\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"/content/drive/MyDrive/brain_tumor_vit_model_for_heatmap\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMulKmWEGdCW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUvg0J0sGgpl"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Example image path - replace with one of your test images\n",
        "image_path = \"/content/drive/MyDrive/test_images/sample-img5-pituitary.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Preprocess using the ViT processor\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nZekhHgGuW_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get predicted class index\n",
        "pred_class_idx = outputs.logits.argmax(-1).item()\n",
        "print(\"Predicted class index:\", pred_class_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL_RnxG9GyWa"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store activations and gradients\n",
        "activations = {}\n",
        "gradients = {}\n",
        "\n",
        "# Forward hook to get activations\n",
        "def forward_hook(module, input, output):\n",
        "    activations[\"value\"] = output\n",
        "\n",
        "# Backward hook to get gradients\n",
        "def backward_hook(module, grad_input, grad_output):\n",
        "    gradients[\"value\"] = grad_output[0]\n",
        "\n",
        "# Register hooks on the last transformer block's output\n",
        "# This is typically where we get meaningful features for Grad-CAM\n",
        "target_layer = model.vit.encoder.layer[-1].output\n",
        "target_layer.register_forward_hook(forward_hook)\n",
        "target_layer.register_full_backward_hook(backward_hook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-gFf5q-G5oh"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Forward pass again (required for backward)\n",
        "outputs = model(**inputs)\n",
        "pred_class_idx = outputs.logits.argmax(-1).item()\n",
        "\n",
        "# Take the score of the predicted class\n",
        "score = outputs.logits[0, pred_class_idx]\n",
        "\n",
        "# Backward pass to compute gradients\n",
        "model.zero_grad()\n",
        "score.backward()\n",
        "\n",
        "# Check shapes of activations and gradients\n",
        "print(\"Activations shape:\", activations[\"value\"].shape)\n",
        "print(\"Gradients shape:\", gradients[\"value\"].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmKj6ogIG_mu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get activations and gradients\n",
        "acts = activations[\"value\"].detach()[0]  # remove batch dimension\n",
        "grads = gradients[\"value\"].detach()[0]\n",
        "\n",
        "# Compute weights: global average pooling of gradients\n",
        "weights = grads.mean(dim=0)  # shape: [hidden_dim]\n",
        "\n",
        "# Weighted sum of activations\n",
        "cam = (acts * weights[None, :]).sum(dim=-1)  # shape: [seq_len]\n",
        "\n",
        "# Remove CLS token (first token) if present\n",
        "cam = cam[1:]  # shape: [196] for 14x14 patches\n",
        "\n",
        "# Reshape to spatial dimensions\n",
        "cam = cam.reshape(14, 14).cpu().numpy()\n",
        "\n",
        "# Normalize\n",
        "cam = np.maximum(cam, 0)\n",
        "cam = cam - cam.min()\n",
        "cam = cam / cam.max()\n",
        "\n",
        "# Resize to original image size\n",
        "cam = cv2.resize(cam, image.size)\n",
        "\n",
        "# Overlay heatmap on original image\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(image)\n",
        "plt.imshow(cam, cmap='jet', alpha=0.5)\n",
        "plt.title(\"Grad-CAM Heatmap\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTiox7dMKyQI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check that your model folder exists\n",
        "!ls -l /content/drive/MyDrive/brain_tumor_vit_model_for_heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_6-cw8jLUgi"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers safetensors timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLNXinmoLdzM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForImageClassification\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model_for_heatmap\"\n",
        "\n",
        "# Load config\n",
        "config = AutoConfig.from_pretrained(model_path)\n",
        "print(\"Model config loaded.\")\n",
        "print(\"Number of labels:\", config.num_labels)\n",
        "\n",
        "# âœ… Load model (no 'from_safetensors' here)\n",
        "vit_model = AutoModelForImageClassification.from_pretrained(model_path)\n",
        "vit_model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Xsy9ZQL44z"
      },
      "outputs": [],
      "source": [
        "# Safe label mapping\n",
        "id2label = {int(k): v for k,v in config.id2label.items()}  # convert keys to int\n",
        "\n",
        "pred_label = id2label.get(pred_class, f\"Label_{pred_class}\")\n",
        "print(\"Predicted label:\", pred_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0hJd9rDMXqS"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchcam matplotlib nibabel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlEHeYa2Mgw8"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchcam matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbfl-TPePskM"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchcam matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP86s97ST5hF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "\n",
        "# Create ViT model with 4 classes (like your trained model)\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4)\n",
        "\n",
        "# Load weights\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_model.pth\"\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pESdPYAD919m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchcam.methods import SmoothGradCAMpp\n",
        "from timm import create_model\n",
        "\n",
        "# Load model\n",
        "model = create_model('vit_base_patch16_224', pretrained=False, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location='cpu'))\n",
        "model.eval()\n",
        "\n",
        "# Load image\n",
        "img_path = \"\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Use SmoothGradCAM++\n",
        "cam_extractor = SmoothGradCAMpp(model)\n",
        "\n",
        "# Forward pass & get prediction\n",
        "out = model(input_tensor)\n",
        "pred_class = out.argmax(dim=1).item()\n",
        "\n",
        "# Extract CAM\n",
        "activation_map = cam_extractor(pred_class, out)\n",
        "\n",
        "# Convert CAM to heatmap\n",
        "heatmap = activation_map[0].detach().numpy()\n",
        "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "\n",
        "# Overlay heatmap on original image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(img)\n",
        "plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GRIbFsu96eo"
      },
      "outputs": [],
      "source": [
        "print(attentions[-1].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTiAqWtv-RZN"
      },
      "outputs": [],
      "source": [
        "!pip install torchcam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki0rIPEU-kMq"
      },
      "outputs": [],
      "source": [
        "heatmap_tensor = heatmap_tensor.mean(axis=-1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSFHJd06_Reu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "heatmap_tensor = torch.tensor(heatmap_tensor)  # convert from NumPy/list to tensor\n",
        "heatmap_tensor = heatmap_tensor.mean(dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bejQ9pJ_cai"
      },
      "outputs": [],
      "source": [
        "print(\"heatmap_tensor shape:\", heatmap_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B86_jael_oSl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nykGaPxX_z3W"
      },
      "outputs": [],
      "source": [
        "img_path = \"/content/drive/MyDrive/test_images/sample-img6-pituitary.jpg\"  # replace with your file\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "img_np = np.array(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8f0zLmN_4Bo"
      },
      "outputs": [],
      "source": [
        "# 1ï¸âƒ£ Make sure input requires grad\n",
        "input_tensor.requires_grad_(True)\n",
        "\n",
        "# 2ï¸âƒ£ Define a hook to capture attention / hidden states\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output\n",
        "    return hook\n",
        "\n",
        "# 3ï¸âƒ£ Register hook on the ViT layer you want (e.g., last encoder block)\n",
        "model.blocks[-1].register_forward_hook(get_activation('last_block'))\n",
        "\n",
        "# 4ï¸âƒ£ Forward pass\n",
        "outputs = model(input_tensor)  # no torch.no_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG7yX1c9AAUB"
      },
      "outputs": [],
      "source": [
        "heatmap_tensor = activation['last_block']  # shape: (1, num_patches, hidden_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T1aCEXFAEzJ"
      },
      "outputs": [],
      "source": [
        "print(\"Original heatmap_tensor shape:\", heatmap_tensor.shape)\n",
        "# Expect something like: torch.Size([1, 196, 768])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDil0aA0AH7j"
      },
      "outputs": [],
      "source": [
        "if len(heatmap_tensor.shape) == 3:  # (batch, num_patches, hidden_dim)\n",
        "    heatmap_tensor = heatmap_tensor.mean(dim=-1)  # shape: (batch, num_patches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jY54Vc7AJMR"
      },
      "outputs": [],
      "source": [
        "if heatmap_tensor.shape[0] == 1:  # batch size = 1\n",
        "    heatmap_tensor = heatmap_tensor.squeeze(0)  # shape: (num_patches,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0PYtrjgAKk5"
      },
      "outputs": [],
      "source": [
        "# Remove CLS token\n",
        "heatmap_tensor_no_cls = heatmap_tensor[1:]  # shape (196,)\n",
        "\n",
        "num_patches = heatmap_tensor_no_cls.shape[0]  # 196\n",
        "size = int(num_patches ** 0.5)  # 14\n",
        "\n",
        "# Reshape to 2D\n",
        "heatmap_2d = heatmap_tensor_no_cls.reshape(size, size).detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx_M9z9LAVwD"
      },
      "outputs": [],
      "source": [
        "heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SYoT0s7AXsD"
      },
      "outputs": [],
      "source": [
        "heatmap_img = Image.fromarray(np.uint8(heatmap_2d * 255)).resize(\n",
        "    (img_np.shape[1], img_np.shape[0]), resample=Image.BILINEAR\n",
        ")\n",
        "heatmap_img = np.array(heatmap_img) / 255  # normalize again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O-ZuDQ1AZZb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(img_np)\n",
        "plt.imshow(heatmap_img, cmap='jet', alpha=0.5)  # alpha controls transparency\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNd2zl6OBAl_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Remove the first token if it is CLS\n",
        "heatmap_tensor = heatmap_tensor[1:]   # removes 1 element, size now 196\n",
        "num_patches = heatmap_tensor.shape[0]\n",
        "grid_size = int(num_patches ** 0.5)   # now 14\n",
        "heatmap_2d = heatmap_tensor.reshape(grid_size, grid_size).detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIxP1qnHBJjb"
      },
      "outputs": [],
      "source": [
        "heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9_jzByvBLu7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# If img is a PIL Image\n",
        "img_np = np.array(img)         # shape will be (H, W, C) for RGB\n",
        "\n",
        "H, W = img_np.shape[:2]        # now it works\n",
        "heatmap_resized = cv2.resize(heatmap_2d, (W, H))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS-5e_TyBUNh"
      },
      "outputs": [],
      "source": [
        "heatmap_color = cv2.applyColorMap((heatmap_resized*255).astype('uint8'), cv2.COLORMAP_JET)\n",
        "heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "alpha = 0.5\n",
        "overlay = cv2.addWeighted(img_np, 1-alpha, heatmap_color, alpha, 0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(overlay)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW6htVMyDcFq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTxtCERzDeMe"
      },
      "outputs": [],
      "source": [
        "# If your image is PIL Image\n",
        "img = Image.open(\"/content/drive/MyDrive/test_images/sample-img3-glioma.jpg\").convert(\"RGB\")  # ensure 3 channels\n",
        "img_np = np.array(img)   # shape: H x W x 3\n",
        "H, W = img_np.shape[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwmTu60aDkwR"
      },
      "outputs": [],
      "source": [
        "# Assume heatmap_tensor is 1D (num_patches + 1 if CLS included)\n",
        "if heatmap_tensor.shape[0] > 0 and int((heatmap_tensor.shape[0]-1)**0.5)**2 == heatmap_tensor.shape[0]-1:\n",
        "    heatmap_tensor = heatmap_tensor[1:]   # remove CLS token if needed\n",
        "\n",
        "num_patches = heatmap_tensor.shape[0]\n",
        "grid_size = int(num_patches ** 0.5)\n",
        "heatmap_2d = heatmap_tensor.reshape(grid_size, grid_size).detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JVw8r7HDl_d"
      },
      "outputs": [],
      "source": [
        "heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQhdnPmwDnRm"
      },
      "outputs": [],
      "source": [
        "heatmap_resized = cv2.resize(heatmap_2d, (W, H))  # width, height order in cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4btYZXCDoZl"
      },
      "outputs": [],
      "source": [
        "heatmap_color = cv2.applyColorMap((heatmap_resized*255).astype('uint8'), cv2.COLORMAP_JET)\n",
        "heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QeQHuRkDpeg"
      },
      "outputs": [],
      "source": [
        "alpha = 0.5\n",
        "overlay = cv2.addWeighted(img_np, 1-alpha, heatmap_color, alpha, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q025piiVDqiO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(overlay)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfPtP9aTD9fh"
      },
      "outputs": [],
      "source": [
        "threshold = 0.6   # adjust between 0.4â€“0.8\n",
        "tumor_mask = (heatmap_resized > threshold).astype('uint8') * 255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePyg0ltLD--Z"
      },
      "outputs": [],
      "source": [
        "kernel = np.ones((5,5), np.uint8)\n",
        "tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_OPEN, kernel)\n",
        "tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_CLOSE, kernel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW9zEIc0EBGk"
      },
      "outputs": [],
      "source": [
        "# Convert mask to 3-channel\n",
        "tumor_mask_color = cv2.cvtColor(tumor_mask, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "# Highlight tumor in red\n",
        "highlighted = img_np.copy()\n",
        "highlighted[tumor_mask > 0] = [255, 0, 0]   # mark tumor pixels red\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1fUyMVSECjq"
      },
      "outputs": [],
      "source": [
        "# Part 1: compute and save heatmap_resized.npy and img_np.npy\n",
        "# ---------------------------------------------------------\n",
        "import torch, math, os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---- User inputs ----\n",
        "image_path = \"/content/drive/MyDrive/test_images/sample-img3-glioma.jpg\"   # <-- replace with your file path\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "# ----------------------\n",
        "\n",
        "# load image (PIL -> numpy)\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "img_np = np.array(img)   # H x W x 3 (RGB)\n",
        "H, W = img_np.shape[:2]\n",
        "print(\"Loaded image:\", image_path, \"size:\", (H, W))\n",
        "\n",
        "# load ViT with attentions enabled\n",
        "print(\"Loading model (this may download weights if not cached)...\")\n",
        "model = ViTModel.from_pretrained(model_name, output_attentions=True).to(device).eval()\n",
        "processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# prepare inputs\n",
        "inputs = processor(images=img, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# forward pass -> attentions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    attentions = outputs.attentions\n",
        "    if attentions is None:\n",
        "        raise RuntimeError(\"`attentions` is None. Ensure model loaded with output_attentions=True.\")\n",
        "    # attentions[-1]: (batch, heads, seq_len, seq_len)\n",
        "    last_attn = attentions[-1]                  # last layer\n",
        "    attn_mean = last_attn.mean(dim=1)           # average over heads -> (batch, seq_len, seq_len)\n",
        "    attn_mat = attn_mean[0].cpu()               # (seq_len, seq_len) for batch 0\n",
        "\n",
        "# get CLS -> patch attention (drop CLS itself)\n",
        "cls_to_patches = attn_mat[0, 1:]               # shape: num_patches\n",
        "num_patches = cls_to_patches.shape[0]\n",
        "grid_size = int(math.sqrt(num_patches))\n",
        "if grid_size * grid_size != num_patches:\n",
        "    raise ValueError(f\"num_patches={num_patches} is not a perfect square (grid_size^2).\")\n",
        "\n",
        "# reshape into 2D patch grid\n",
        "heatmap_2d = cls_to_patches.reshape(grid_size, grid_size).numpy()\n",
        "\n",
        "# normalize to 0..1\n",
        "heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n",
        "\n",
        "# resize heatmap to original image size (W,H order for cv2)\n",
        "heatmap_resized = cv2.resize(heatmap_2d, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# save outputs to disk for step-by-step workflow\n",
        "np.save(\"heatmap_resized.npy\", heatmap_resized)\n",
        "np.save(\"img_np.npy\", img_np)\n",
        "np.save(\"heatmap_2d.npy\", heatmap_2d)\n",
        "\n",
        "print(\"Saved: heatmap_resized.npy, img_np.npy, heatmap_2d.npy\")\n",
        "print(\"You can now run Part 2 (post-processing + display).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuxvRxsQGf1-"
      },
      "outputs": [],
      "source": [
        "# Part 2: load heatmap + image, threshold, clean, highlight & save outputs\n",
        "# ------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# ---- Parameters you can tweak ----\n",
        "threshold = 0.6        # start 0.4..0.8. lower => more area, higher => smaller/high-confidence area\n",
        "kernel_size = 5        # morphological cleanup kernel\n",
        "keep_largest_component = True   # if True, only keep the largest connected region (helps remove noise)\n",
        "# ----------------------------------\n",
        "\n",
        "# load saved files from Part 1\n",
        "heatmap_resized = np.load(\"heatmap_resized.npy\")\n",
        "img_np = np.load(\"img_np.npy\")\n",
        "H, W = img_np.shape[:2]\n",
        "print(\"Loaded heatmap and image shapes:\", heatmap_resized.shape, img_np.shape)\n",
        "\n",
        "# visualize the raw heatmap to pick a good threshold (optional)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.title(\"Heatmap (resized) - inspect to choose threshold\")\n",
        "plt.imshow(heatmap_resized, cmap=\"jet\")\n",
        "plt.colorbar(fraction=0.045)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# threshold to create binary mask\n",
        "mask = (heatmap_resized > threshold).astype(\"uint8\") * 255\n",
        "\n",
        "# morphological cleaning\n",
        "kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# optionally keep only the largest connected component\n",
        "if keep_largest_component:\n",
        "    contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if len(contours) > 0:\n",
        "        largest = max(contours, key=cv2.contourArea)\n",
        "        large_mask = np.zeros_like(mask)\n",
        "        cv2.drawContours(large_mask, [largest], -1, 255, thickness=cv2.FILLED)\n",
        "        mask = large_mask\n",
        "    else:\n",
        "        print(\"Warning: no contours found after thresholding. Try lowering the threshold.\")\n",
        "\n",
        "# create a blended red highlight for mask area\n",
        "highlighted = img_np.copy().astype(\"uint8\")\n",
        "red = np.array([255, 0, 0], dtype=np.uint8)\n",
        "\n",
        "# blend red color on mask pixels to preserve some original structure\n",
        "alpha = 0.4  # blending factor for red\n",
        "mask_bool = mask > 0\n",
        "highlighted[mask_bool] = (alpha * red + (1 - alpha) * highlighted[mask_bool]).astype(np.uint8)\n",
        "\n",
        "# show results\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,3,1); plt.title(\"Original MRI\"); plt.imshow(img_np); plt.axis(\"off\")\n",
        "plt.subplot(1,3,2); plt.title(f\"Tumor Mask (thr={threshold})\"); plt.imshow(mask, cmap=\"gray\"); plt.axis(\"off\")\n",
        "plt.subplot(1,3,3); plt.title(\"Tumor Highlighted\"); plt.imshow(highlighted); plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# save outputs\n",
        "cv2.imwrite(\"tumor_mask.png\", mask)   # grayscale\n",
        "# convert RGB->BGR for OpenCV saving\n",
        "cv2.imwrite(\"tumor_highlighted.png\", cv2.cvtColor(highlighted, cv2.COLOR_RGB2BGR))\n",
        "print(\"Saved tumor_mask.png and tumor_highlighted.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma8R9KXMG9qt"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model = ViTForImageClassification.from_pretrained(model_path)\n",
        "\n",
        "# Load processor from base pretrained ViT\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xsTgCSFH6AS"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt-bas_WIG9W"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model = ViTForImageClassification.from_pretrained(model_path, output_attentions=True)\n",
        "\n",
        "# Load the matching processor\n",
        "processor = ViTImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "print(\"Model and processor loaded successfully âœ…\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2lNOgNHILDE"
      },
      "outputs": [],
      "source": [
        "import torch, math\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 1: Load Model + Labels\n",
        "# --------------------------\n",
        "# Replace with your fine-tuned ViT model\n",
        "model_name = \"/content/drive/MyDrive/brain_tumor_vit_model\"   # e.g., \"kamalikaa/brain-tumor-vit\"\n",
        "model = ViTForImageClassification.from_pretrained(model_name, output_attentions=True).to(device).eval()\n",
        "processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Define your class names (update to match your dataset)\n",
        "class_names = [\"glioma\", \"meningioma\", \"pituitary\", \"no_tumor\"]\n",
        "\n",
        "# --------------------------\n",
        "# STEP 2: Load MRI Image\n",
        "# --------------------------\n",
        "image_path = \"\"   # ðŸ”¹ Replace with the path\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "img_np = np.array(img)\n",
        "H, W = img_np.shape[:2]\n",
        "\n",
        "# --------------------------\n",
        "# STEP 3: Prediction\n",
        "# --------------------------\n",
        "inputs = processor(images=img, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    pred_class = torch.argmax(logits, dim=-1).item()\n",
        "    pred_label = class_names[pred_class]\n",
        "    print(\"Predicted Class:\", pred_label)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 4: Attention â†’ Heatmap\n",
        "# --------------------------\n",
        "attentions = outputs.attentions\n",
        "if attentions is None:\n",
        "    raise RuntimeError(\"Attention maps are None. Ensure output_attentions=True in model.\")\n",
        "\n",
        "last_attn = attentions[-1]              # last layer\n",
        "attn_mean = last_attn.mean(dim=1)[0]    # avg over heads â†’ (tokens, tokens)\n",
        "cls_to_patches = attn_mean[0, 1:]       # CLS â†’ patch attention\n",
        "num_patches = cls_to_patches.shape[0]\n",
        "grid_size = int(math.sqrt(num_patches))\n",
        "heatmap_2d = cls_to_patches.reshape(grid_size, grid_size).cpu().numpy()\n",
        "\n",
        "# Normalize + Resize\n",
        "heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n",
        "heatmap_resized = cv2.resize(heatmap_2d, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 5: Tumor Mask\n",
        "# --------------------------\n",
        "threshold = 0.6\n",
        "tumor_mask = (heatmap_resized > threshold).astype(\"uint8\") * 255\n",
        "\n",
        "# Morphological cleanup\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_OPEN, kernel)\n",
        "tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 6: Highlight Tumor\n",
        "# --------------------------\n",
        "highlighted = img_np.copy().astype(\"uint8\")\n",
        "red = np.array([255, 0, 0], dtype=np.uint8)\n",
        "alpha = 0.4\n",
        "highlighted[tumor_mask > 0] = (alpha * red + (1 - alpha) * highlighted[tumor_mask > 0]).astype(np.uint8)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 7: Display Results\n",
        "# --------------------------\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"Original MRI\")\n",
        "plt.imshow(img_np)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(f\"Tumor Mask (thr={threshold})\")\n",
        "plt.imshow(tumor_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(f\"Prediction: {pred_label}\")\n",
        "plt.imshow(highlighted)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvjmQImaJ20u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "import math\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Setup\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Path to your fine-tuned ViT model folder\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "\n",
        "# Load model and processor\n",
        "model = ViTForImageClassification.from_pretrained(model_path, output_attentions=True).to(device).eval()\n",
        "processor = ViTImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "# Correct label mapping\n",
        "model.config.id2label = {\n",
        "    0: \"glioma\",\n",
        "    1: \"meningioma\",\n",
        "    2: \"notumor\",\n",
        "    3: \"pituitary\"\n",
        "}\n",
        "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
        "class_names = list(model.config.id2label.values())\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Load MRI Image\n",
        "# -----------------------------\n",
        "image_path = \"/content/drive/MyDrive/test_images/sample-img7-notumor.jpg\"   # <-- replace with your image path\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "img_np = np.array(img)\n",
        "H, W = img_np.shape[:2]\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Preprocess and Predict\n",
        "# -----------------------------\n",
        "inputs = processor(images=img, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    pred_class = torch.argmax(logits, dim=-1).item()\n",
        "    pred_label = class_names[pred_class]\n",
        "    print(\"Predicted Class:\", pred_label)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Attention â†’ Heatmap\n",
        "# -----------------------------\n",
        "attentions = outputs.attentions\n",
        "if attentions is None:\n",
        "    raise RuntimeError(\"Attentions are None. Ensure output_attentions=True\")\n",
        "\n",
        "last_attn = attentions[-1]                 # last layer\n",
        "attn_mean = last_attn.mean(dim=1)[0]       # average over heads â†’ (tokens, tokens)\n",
        "cls_to_patches = attn_mean[0, 1:]          # CLS â†’ patch attention\n",
        "num_patches = cls_to_patches.shape[0]\n",
        "grid_size = int(math.sqrt(num_patches))\n",
        "heatmap_2d = cls_to_patches.reshape(grid_size, grid_size).cpu().numpy()\n",
        "\n",
        "# Normalize + Resize\n",
        "heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n",
        "heatmap_resized = cv2.resize(heatmap_2d, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Create Tumor Mask\n",
        "# -----------------------------\n",
        "threshold = 0.6\n",
        "tumor_mask = (heatmap_resized > threshold).astype(\"uint8\") * 255\n",
        "\n",
        "# Morphological cleanup\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_OPEN, kernel)\n",
        "tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Highlight Tumor on MRI\n",
        "# -----------------------------\n",
        "highlighted = img_np.copy().astype(\"uint8\")\n",
        "red = np.array([255, 0, 0], dtype=np.uint8)\n",
        "alpha = 0.4\n",
        "highlighted[tumor_mask > 0] = (alpha * red + (1 - alpha) * highlighted[tumor_mask > 0]).astype(np.uint8)\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Display Results\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"Original MRI\")\n",
        "plt.imshow(img_np)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(f\"Tumor Mask (thr={threshold})\")\n",
        "plt.imshow(tumor_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(f\"Prediction: {pred_label}\")\n",
        "plt.imshow(highlighted)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Save outputs (optional)\n",
        "# -----------------------------\n",
        "cv2.imwrite(\"tumor_mask.png\", tumor_mask)\n",
        "cv2.imwrite(\"tumor_highlighted.png\", cv2.cvtColor(highlighted, cv2.COLOR_RGB2BGR))\n",
        "print(\"Saved tumor_mask.png and tumor_highlighted.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgSPJ1WWKuxM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "import math\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Setup\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Path to your fine-tuned ViT model folder\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "\n",
        "# Load model and processor\n",
        "model = ViTForImageClassification.from_pretrained(model_path, output_attentions=True).to(device).eval()\n",
        "processor = ViTImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "# Correct label mapping\n",
        "model.config.id2label = {\n",
        "    0: \"glioma\",\n",
        "    1: \"meningioma\",\n",
        "    2: \"notumor\",\n",
        "    3: \"pituitary\"\n",
        "}\n",
        "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
        "class_names = list(model.config.id2label.values())\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Load MRI Image\n",
        "# -----------------------------\n",
        "image_path = \"/content/drive/MyDrive/test_images/sample-img6-pituitary.jpg\"   # <-- replace with your image path\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "img_np = np.array(img)\n",
        "H, W = img_np.shape[:2]\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Preprocess and Predict\n",
        "# -----------------------------\n",
        "inputs = processor(images=img, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    pred_class = torch.argmax(logits, dim=-1).item()\n",
        "    pred_label = class_names[pred_class]\n",
        "    print(\"Predicted Class:\", pred_label)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Generate Tumor Heatmap only if tumor exists\n",
        "# -----------------------------\n",
        "if pred_label == \"notumor\":\n",
        "    print(\"No tumor detected. Skipping heatmap generation.\")\n",
        "    tumor_mask = np.zeros((H,W), dtype=np.uint8)\n",
        "    highlighted = img_np.copy()\n",
        "else:\n",
        "    # Attention â†’ Heatmap\n",
        "    attentions = outputs.attentions\n",
        "    if attentions is None:\n",
        "        raise RuntimeError(\"Attentions are None. Ensure output_attentions=True\")\n",
        "\n",
        "    last_attn = attentions[-1]                 # last layer\n",
        "    attn_mean = last_attn.mean(dim=1)[0]       # average over heads â†’ (tokens, tokens)\n",
        "    cls_to_patches = attn_mean[0, 1:]          # CLS â†’ patch attention\n",
        "    num_patches = cls_to_patches.shape[0]\n",
        "    grid_size = int(math.sqrt(num_patches))\n",
        "    heatmap_2d = cls_to_patches.reshape(grid_size, grid_size).cpu().numpy()\n",
        "\n",
        "    # Normalize + Resize\n",
        "    heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n",
        "    heatmap_resized = cv2.resize(heatmap_2d, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Threshold â†’ Tumor Mask\n",
        "    threshold = 0.6\n",
        "    tumor_mask = (heatmap_resized > threshold).astype(\"uint8\") * 255\n",
        "\n",
        "    # Morphological cleanup\n",
        "    kernel = np.ones((5,5), np.uint8)\n",
        "    tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_OPEN, kernel)\n",
        "    tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Highlight Tumor on MRI\n",
        "    highlighted = img_np.copy().astype(\"uint8\")\n",
        "    red = np.array([255, 0, 0], dtype=np.uint8)\n",
        "    alpha = 0.4\n",
        "    highlighted[tumor_mask > 0] = (alpha * red + (1 - alpha) * highlighted[tumor_mask > 0]).astype(np.uint8)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Display Results\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"Original MRI\")\n",
        "plt.imshow(img_np)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(f\"Tumor Mask (thr={threshold if pred_label!='notumor' else 'N/A'})\")\n",
        "plt.imshow(tumor_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(f\"Prediction: {pred_label}\")\n",
        "plt.imshow(highlighted)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Save outputs (optional)\n",
        "# -----------------------------\n",
        "cv2.imwrite(\"tumor_mask.png\", tumor_mask)\n",
        "cv2.imwrite(\"tumor_highlighted.png\", cv2.cvtColor(highlighted, cv2.COLOR_RGB2BGR))\n",
        "print(\"Saved tumor_mask.png and tumor_highlighted.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbHN5mo2LVyz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "import math\n",
        "from google.colab import files\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Setup\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Path to your fine-tuned ViT model folder\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "\n",
        "# Load model and processor\n",
        "model = ViTForImageClassification.from_pretrained(model_path, output_attentions=True).to(device).eval()\n",
        "processor = ViTImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "# Correct label mapping\n",
        "model.config.id2label = {\n",
        "    0: \"glioma\",\n",
        "    1: \"meningioma\",\n",
        "    2: \"notumor\",\n",
        "    3: \"pituitary\"\n",
        "}\n",
        "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
        "class_names = list(model.config.id2label.values())\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Upload MRI Image\n",
        "# -----------------------------\n",
        "uploaded = files.upload()  # Opens file picker in Colab\n",
        "image_path = list(uploaded.keys())[0]\n",
        "print(\"Uploaded file:\", image_path)\n",
        "\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "img_np = np.array(img)\n",
        "H, W = img_np.shape[:2]\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Preprocess and Predict\n",
        "# -----------------------------\n",
        "inputs = processor(images=img, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    pred_class = torch.argmax(logits, dim=-1).item()\n",
        "    pred_label = class_names[pred_class]\n",
        "    print(\"Predicted Class:\", pred_label)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Generate Tumor Heatmap only if tumor exists\n",
        "# -----------------------------\n",
        "if pred_label == \"notumor\":\n",
        "    print(\"No tumor detected. Skipping heatmap generation.\")\n",
        "    tumor_mask = np.zeros((H,W), dtype=np.uint8)\n",
        "    highlighted = img_np.copy()\n",
        "else:\n",
        "    # Attention â†’ Heatmap\n",
        "    attentions = outputs.attentions\n",
        "    if attentions is None:\n",
        "        raise RuntimeError(\"Attentions are None. Ensure output_attentions=True\")\n",
        "\n",
        "    last_attn = attentions[-1]                 # last layer\n",
        "    attn_mean = last_attn.mean(dim=1)[0]       # average over heads â†’ (tokens, tokens)\n",
        "    cls_to_patches = attn_mean[0, 1:]          # CLS â†’ patch attention\n",
        "    num_patches = cls_to_patches.shape[0]\n",
        "    grid_size = int(math.sqrt(num_patches))\n",
        "    heatmap_2d = cls_to_patches.reshape(grid_size, grid_size).cpu().numpy()\n",
        "\n",
        "    # Normalize + Resize\n",
        "    heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n",
        "    heatmap_resized = cv2.resize(heatmap_2d, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Threshold â†’ Tumor Mask\n",
        "    threshold = 0.6\n",
        "    tumor_mask = (heatmap_resized > threshold).astype(\"uint8\") * 255\n",
        "\n",
        "    # Morphological cleanup\n",
        "    kernel = np.ones((5,5), np.uint8)\n",
        "    tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_OPEN, kernel)\n",
        "    tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Highlight Tumor on MRI\n",
        "    highlighted = img_np.copy().astype(\"uint8\")\n",
        "    red = np.array([255, 0, 0], dtype=np.uint8)\n",
        "    alpha = 0.4\n",
        "    highlighted[tumor_mask > 0] = (alpha * red + (1 - alpha) * highlighted[tumor_mask > 0]).astype(np.uint8)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Display Results\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"Original MRI\")\n",
        "plt.imshow(img_np)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(f\"Tumor Mask (thr={threshold if pred_label!='notumor' else 'N/A'})\")\n",
        "plt.imshow(tumor_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(f\"Prediction: {pred_label}\")\n",
        "plt.imshow(highlighted)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Save outputs\n",
        "# -----------------------------\n",
        "cv2.imwrite(\"tumor_mask.png\", tumor_mask)\n",
        "cv2.imwrite(\"tumor_highlighted.png\", cv2.cvtColor(highlighted, cv2.COLOR_RGB2BGR))\n",
        "print(\"Saved tumor_mask.png and tumor_highlighted.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWYdwIvFOskN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "import math\n",
        "import gradio as gr\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Setup model\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "model = ViTForImageClassification.from_pretrained(model_path, output_attentions=True).to(device).eval()\n",
        "processor = ViTImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "model.config.id2label = {0: \"glioma\", 1: \"meningioma\", 2: \"notumor\", 3: \"pituitary\"}\n",
        "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
        "class_names = list(model.config.id2label.values())\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Prediction + Highlighting function\n",
        "# -----------------------------\n",
        "def predict_and_highlight(image: Image.Image, threshold: float = 0.6):\n",
        "    img_np = np.array(image)\n",
        "    H, W = img_np.shape[:2]\n",
        "\n",
        "    # Preprocess\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred_class = torch.argmax(logits, dim=-1).item()\n",
        "        pred_label = class_names[pred_class]\n",
        "\n",
        "    # Generate tumor mask\n",
        "    if pred_label == \"notumor\":\n",
        "        tumor_mask = np.zeros((H, W), dtype=np.uint8)\n",
        "        highlighted = img_np.copy()\n",
        "    else:\n",
        "        attentions = outputs.attentions\n",
        "        last_attn = attentions[-1]\n",
        "        attn_mean = last_attn.mean(dim=1)[0]\n",
        "        cls_to_patches = attn_mean[0, 1:]\n",
        "        num_patches = cls_to_patches.shape[0]\n",
        "        grid_size = int(math.sqrt(num_patches))\n",
        "        heatmap_2d = cls_to_patches.reshape(grid_size, grid_size).cpu().numpy()\n",
        "\n",
        "        # Normalize + Resize\n",
        "        heatmap_2d = (heatmap_2d - heatmap_2d.min()) / (heatmap_2d.max() - heatmap_2d.min() + 1e-8)\n",
        "        heatmap_resized = cv2.resize(heatmap_2d, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Threshold â†’ Tumor Mask\n",
        "        tumor_mask = (heatmap_resized > threshold).astype(\"uint8\") * 255\n",
        "\n",
        "        # Morphological cleanup\n",
        "        kernel = np.ones((5,5), np.uint8)\n",
        "        tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_OPEN, kernel)\n",
        "        tumor_mask = cv2.morphologyEx(tumor_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        # Highlight tumor\n",
        "        highlighted = img_np.copy().astype(\"uint8\")\n",
        "        red = np.array([255, 0, 0], dtype=np.uint8)\n",
        "        alpha = 0.4\n",
        "        highlighted[tumor_mask > 0] = (alpha * red + (1 - alpha) * highlighted[tumor_mask > 0]).astype(np.uint8)\n",
        "\n",
        "    return pred_label, img_np, tumor_mask, highlighted\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Polished Gradio UI\n",
        "# -----------------------------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ§  Brain Tumor Classification & Highlighting\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload MRI Image\")\n",
        "            threshold_slider = gr.Slider(0.1, 1.0, value=0.6, step=0.05, label=\"Attention Threshold\")\n",
        "        with gr.Column(scale=2):\n",
        "            prediction_output = gr.Textbox(label=\"Prediction\", interactive=False)\n",
        "            with gr.Row():\n",
        "                img1 = gr.Image(type=\"numpy\", label=\"Original MRI\")\n",
        "                img2 = gr.Image(type=\"numpy\", label=\"Tumor Mask\")\n",
        "                img3 = gr.Image(type=\"numpy\", label=\"Highlighted Tumor\")\n",
        "\n",
        "    def run(image, threshold):\n",
        "        pred, orig, mask, highlighted = predict_and_highlight(image, threshold)\n",
        "        return pred, orig, mask, highlighted\n",
        "\n",
        "    image_input.change(run, inputs=[image_input, threshold_slider], outputs=[prediction_output, img1, img2, img3])\n",
        "    threshold_slider.change(run, inputs=[image_input, threshold_slider], outputs=[prediction_output, img1, img2, img3])\n",
        "\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjzVsxL4FlLc"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "!python -m pip --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO5PR9u3OQA_"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EINvRxhtOkW-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pytorch_grad_cam\n",
        "\n",
        "print(\"âœ… PyTorch version:\", torch.__version__)\n",
        "print(\"âœ… Torchvision version:\", torchvision.__version__)\n",
        "print(\"âœ… pytorch-grad-cam imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P68VnhLFOzSA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojF7_BB3TGOB"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "# âœ… Path to your fine-tuned ViT model folder\n",
        "model_path = \"/content/drive/MyDrive/brain_tumor_vit_model_for_heatmap\"\n",
        "\n",
        "# Load the model\n",
        "model = ViTForImageClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# Update class labels\n",
        "model.config.id2label = {\n",
        "    0: \"glioma\",\n",
        "    1: \"meningioma\",\n",
        "    3: \"pituitary\",\n",
        "    2: \"no_tumor\"\n",
        "}\n",
        "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
        "\n",
        "print(\"âœ… Hugging Face ViT model loaded successfully!\")\n",
        "print(\"Classes:\", model.config.id2label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCfTrBIQTTxu"
      },
      "outputs": [],
      "source": [
        "# âœ… Set your image path directly\n",
        "img_path = \"/content/drive/MyDrive/test_images/sample-img3-glioma.jpg\"\n",
        "print(\"âœ… Using image:\", img_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9EpPNBaTcTR"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load and resize image\n",
        "orig_image = Image.open(img_path).convert(\"RGB\")\n",
        "image = orig_image.resize((224, 224))\n",
        "\n",
        "# Preprocess: convert to tensor and normalize (ImageNet stats)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Convert original image to numpy array for overlay\n",
        "rgb_image = np.array(image) / 255.0\n",
        "\n",
        "print(\"âœ… Image preprocessed for Grad-CAM\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdLMkFcKTh4V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HuggingFaceViTWrapper(nn.Module):\n",
        "    def __init__(self, hf_model):\n",
        "        super().__init__()\n",
        "        self.model = hf_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Return only logits tensor\n",
        "        return self.model(x).logits\n",
        "\n",
        "# Wrap the model\n",
        "wrapped_model = HuggingFaceViTWrapper(model)\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "\n",
        "# Confirm wrapper\n",
        "print(\"âœ… Model wrapped for Grad-CAM\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9jSeQbMT-MQ"
      },
      "outputs": [],
      "source": [
        "# --- Ensure device matches ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "\n",
        "input_tensor = input_tensor.to(device)  # move input to same device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23DvHjZ8VWet"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = wrapped_model(input_tensor)\n",
        "pred_class = outputs.argmax(dim=1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[pred_class])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVsABzwuVrCR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import numpy as np\n",
        "\n",
        "# --- 1ï¸âƒ£ Device setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "input_tensor = input_tensor.to(device)\n",
        "\n",
        "# --- 2ï¸âƒ£ Select target layer (last patch embedding conv layer) ---\n",
        "target_layers = [wrapped_model.model.vit.embeddings.patch_embeddings.projection]\n",
        "\n",
        "# --- 3ï¸âƒ£ Create GradCAM object ---\n",
        "cam = GradCAM(model=wrapped_model, target_layers=target_layers)\n",
        "\n",
        "# --- 4ï¸âƒ£ Predict class ---\n",
        "with torch.no_grad():\n",
        "    outputs = wrapped_model(input_tensor)\n",
        "pred_class = outputs.argmax(dim=1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[pred_class])\n",
        "\n",
        "# --- 5ï¸âƒ£ Prepare target for Grad-CAM ---\n",
        "targets = [ClassifierOutputTarget(pred_class)]\n",
        "\n",
        "# --- 6ï¸âƒ£ Generate heatmap ---\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
        "cam_image = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True)\n",
        "\n",
        "# --- 7ï¸âƒ£ Display heatmap ---\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cam_image)\n",
        "plt.axis('off')\n",
        "plt.title(f\"Grad-CAM Heatmap - {model.config.id2label[pred_class]}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR4EPab7WukT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM, EigenCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# --- 1ï¸âƒ£ Device setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "input_tensor = input_tensor.to(device)\n",
        "\n",
        "# --- 2ï¸âƒ£ Select target layer (last patch embedding conv layer) ---\n",
        "target_layer = wrapped_model.model.vit.embeddings.patch_embeddings.projection\n",
        "\n",
        "# --- 3ï¸âƒ£ Predict class ---\n",
        "with torch.no_grad():\n",
        "    outputs = wrapped_model(input_tensor)\n",
        "pred_class = outputs.argmax(dim=1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[pred_class])\n",
        "\n",
        "# --- 4ï¸âƒ£ Prepare target ---\n",
        "targets = [ClassifierOutputTarget(pred_class)]\n",
        "\n",
        "# --- 5ï¸âƒ£ GradCAM & EigenCAM ---\n",
        "gradcam = GradCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "eigencam = EigenCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "\n",
        "grayscale_grad = gradcam(input_tensor=input_tensor, targets=targets)[0]\n",
        "grayscale_eigen = eigencam(input_tensor=input_tensor, targets=targets)[0]\n",
        "\n",
        "# --- 6ï¸âƒ£ Average and normalize ---\n",
        "grayscale_cam = (grayscale_grad + grayscale_eigen) / 2\n",
        "grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min())\n",
        "\n",
        "# --- 7ï¸âƒ£ Threshold + Morphological cleaning ---\n",
        "mask = (grayscale_cam >= 0.45).astype('uint8') * 255\n",
        "mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((5,5),np.uint8))\n",
        "mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((7,7),np.uint8))\n",
        "\n",
        "# --- 8ï¸âƒ£ Keep largest contour ---\n",
        "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "mask_clean = np.zeros_like(mask)\n",
        "if contours:\n",
        "    largest = max(contours, key=cv2.contourArea)\n",
        "    cv2.drawContours(mask_clean, [largest], -1, 255, -1)\n",
        "\n",
        "# --- 9ï¸âƒ£ Overlay heatmap on original image ---\n",
        "cam_image = show_cam_on_image(rgb_image, mask_clean/255.0, use_rgb=True)\n",
        "\n",
        "# --- ðŸ”Ÿ Display results ---\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cam_image)\n",
        "plt.axis('off')\n",
        "plt.title(f\"CAM Overlay - {model.config.id2label[pred_class]}\")\n",
        "plt.show()\n",
        "\n",
        "# --- 1ï¸âƒ£1ï¸âƒ£ Optional: save overlay, mask, and bounding box ---\n",
        "cv2.imwrite(\"cam_overlay.png\", cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR))\n",
        "cv2.imwrite(\"tumor_mask.png\", mask_clean)\n",
        "if contours:\n",
        "    x,y,w,h = cv2.boundingRect(largest)\n",
        "    print(f\"Bounding Box: x={x}, y={y}, w={w}, h={h}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZOPR6wFhEWI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekcrT17RhGEw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXTHtu-vW70S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from pytorch_grad_cam import GradCAM, EigenCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# Make sure your model is loaded and wrapped_model is ready\n",
        "# wrapped_model = ... (your ViT model)\n",
        "# model.config.id2label = {0: \"Glioma\", 1: \"Meningioma\", 2: \"Pituitary\"}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "\n",
        "target_layer = wrapped_model.model.vit.embeddings.patch_embeddings.projection\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"Convert PIL image to model tensor\"\"\"\n",
        "    img = img.resize((224,224))  # change if model uses other resolution\n",
        "    rgb_image = np.array(img)/255.0\n",
        "    input_tensor = torch.tensor(rgb_image).permute(2,0,1).unsqueeze(0).float()\n",
        "    return input_tensor.to(device), rgb_image\n",
        "\n",
        "def generate_heatmap(img):\n",
        "    input_tensor, rgb_image = preprocess_image(img)\n",
        "\n",
        "    # --- Predict ---\n",
        "    with torch.no_grad():\n",
        "        outputs = wrapped_model(input_tensor)\n",
        "    pred_class = outputs.argmax(dim=1).item()\n",
        "    label = model.config.id2label[pred_class]\n",
        "\n",
        "    # --- CAMs ---\n",
        "    targets = [ClassifierOutputTarget(pred_class)]\n",
        "    gradcam = GradCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "    eigencam = EigenCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "\n",
        "    grayscale_grad = gradcam(input_tensor=input_tensor, targets=targets)[0]\n",
        "    grayscale_eigen = eigencam(input_tensor=input_tensor, targets=targets)[0]\n",
        "\n",
        "    # --- Average & normalize ---\n",
        "    grayscale_cam = (grayscale_grad + grayscale_eigen)/2\n",
        "    grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min())\n",
        "\n",
        "    # --- Threshold + morphology ---\n",
        "    mask = (grayscale_cam >= 0.45).astype('uint8')*255\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((5,5),np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((7,7),np.uint8))\n",
        "\n",
        "    # --- Keep largest contour ---\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    mask_clean = np.zeros_like(mask)\n",
        "    if contours:\n",
        "        largest = max(contours, key=cv2.contourArea)\n",
        "        cv2.drawContours(mask_clean, [largest], -1, 255, -1)\n",
        "\n",
        "    # --- Overlay ---\n",
        "    cam_image = show_cam_on_image(rgb_image, mask_clean/255.0, use_rgb=True)\n",
        "\n",
        "    return label, cam_image\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "interface = gr.Interface(\n",
        "    fn=generate_heatmap,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Text(label=\"Predicted Class\"), gr.Image(label=\"Tumor Heatmap\")],\n",
        "    title=\"Brain Tumor Prediction & Heatmap\",\n",
        "    description=\"Upload an MRI image. The model predicts the tumor type and shows a cleaned GradCAM+EigenCAM heatmap.\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYVPK64lXUzA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from pytorch_grad_cam import GradCAM, EigenCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "target_layer = wrapped_model.model.vit.embeddings.patch_embeddings.projection\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = img.resize((224,224))  # adjust for your model\n",
        "    rgb_image = np.array(img)/255.0\n",
        "    input_tensor = torch.tensor(rgb_image).permute(2,0,1).unsqueeze(0).float()\n",
        "    return input_tensor.to(device), rgb_image\n",
        "\n",
        "def generate_heatmap(img):\n",
        "    input_tensor, rgb_image = preprocess_image(img)\n",
        "\n",
        "    # --- Predict ---\n",
        "    with torch.no_grad():\n",
        "        outputs = wrapped_model(input_tensor)\n",
        "    pred_class = outputs.argmax(dim=1).item()\n",
        "    label = model.config.id2label[pred_class]\n",
        "\n",
        "    # --- CAMs ---\n",
        "    targets = [ClassifierOutputTarget(pred_class)]\n",
        "    gradcam = GradCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "    eigencam = EigenCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "\n",
        "    grayscale_grad = gradcam(input_tensor=input_tensor, targets=targets)[0]\n",
        "    grayscale_eigen = eigencam(input_tensor=input_tensor, targets=targets)[0]\n",
        "\n",
        "    # --- Average & normalize for heatmap ---\n",
        "    grayscale_cam = (grayscale_grad + grayscale_eigen)/2\n",
        "    grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min())\n",
        "\n",
        "    # --- Overlay heatmap on image ---\n",
        "    cam_image = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    return label, cam_image\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=generate_heatmap,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Text(label=\"Predicted Class\"), gr.Image(label=\"Tumor Heatmap\")],\n",
        "    title=\"Brain Tumor Prediction & Heatmap\",\n",
        "    description=\"Upload an MRI image. Shows predicted tumor class and true GradCAM+EigenCAM heatmap overlay.\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3DPFBuWX5jN"
      },
      "outputs": [],
      "source": [
        "print(\"Original input tensor shape:\", input_tensor.shape)\n",
        "print(\"Input tensor ndim:\", input_tensor.ndim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3-2serAeiqQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_tensor = input_tensor.float().to(device)\n",
        "print(\"Tensor device:\", input_tensor.device)\n",
        "print(\"Tensor dtype:\", input_tensor.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcppYWdAenbC"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    target_class = output.argmax(dim=1).item()\n",
        "\n",
        "print(\"Predicted class index:\", target_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR8T5Qsher1C"
      },
      "outputs": [],
      "source": [
        "print(model)  # full model structure\n",
        "print(type(model))  # class of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZwLqz_7fIt1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pytorch_grad_cam import LayerCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Ensure input tensor is float and on device\n",
        "input_tensor = input_tensor.float().to(device)\n",
        "\n",
        "# Forward pass to get predicted class\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    target_class = output.argmax(dim=1).item()\n",
        "print(\"Predicted class index:\", target_class)\n",
        "\n",
        "# Use patch embedding conv layer as target\n",
        "target_layer = model.model.vit.embeddings.patch_embeddings.projection\n",
        "\n",
        "# Initialize LayerCAM\n",
        "cam = LayerCAM(model=model, target_layers=[target_layer])\n",
        "target = ClassifierOutputTarget(target_class)\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=[target])\n",
        "\n",
        "# Extract first image CAM and normalize\n",
        "cam_output = grayscale_cam[0]\n",
        "cam_output = (cam_output - cam_output.min()) / (cam_output.max() - cam_output.min() + 1e-8)\n",
        "\n",
        "# Threshold to create binary mask\n",
        "threshold = 0.5\n",
        "binary_mask = (cam_output > threshold).astype(np.uint8)\n",
        "\n",
        "print(\"Part 1 complete: Binary mask created\")\n",
        "print(\"Binary mask shape:\", binary_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yNAgo92fN2W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "from pytorch_grad_cam import LayerCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from torchvision import transforms\n",
        "from skimage.transform import resize\n",
        "import cv2\n",
        "\n",
        "# Load your Hugging Face ViT wrapper\n",
        "# Example: model = HuggingFaceViTWrapper(...)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_image(img):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    tensor = transform(img)\n",
        "    if tensor.ndim == 2:\n",
        "        tensor = tensor.unsqueeze(0)  # H x W -> 1 x H x W\n",
        "    if tensor.shape[0] not in [1, 3]:\n",
        "        tensor = tensor.permute(2, 0, 1)\n",
        "    tensor = tensor.unsqueeze(0) if tensor.ndim == 3 else tensor\n",
        "    return tensor.float().to(device)\n",
        "\n",
        "# Function to compute focused tumor overlay\n",
        "def highlight_tumor_focused(img, threshold=0.75):\n",
        "    # Preprocess\n",
        "    input_tensor = preprocess_image(img)\n",
        "\n",
        "    # Forward pass to get predicted class\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        target_class = output.argmax(dim=1).item()\n",
        "\n",
        "    # Target layer: patch embedding Conv2d\n",
        "    target_layer = model.model.vit.embeddings.patch_embeddings.projection\n",
        "\n",
        "    # Compute LayerCAM\n",
        "    cam = LayerCAM(model=model, target_layers=[target_layer])\n",
        "    target = ClassifierOutputTarget(target_class)\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=[target])\n",
        "\n",
        "    # Normalize and threshold\n",
        "    cam_output = np.clip(grayscale_cam[0], 0, 1)\n",
        "    binary_mask = (cam_output > threshold).astype(np.uint8)\n",
        "\n",
        "    # Morphological filtering\n",
        "    binary_mask = cv2.erode(binary_mask, np.ones((3,3), np.uint8), iterations=1)\n",
        "    binary_mask = cv2.dilate(binary_mask, np.ones((3,3), np.uint8), iterations=2)\n",
        "\n",
        "    # Keep largest connected component\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, connectivity=8)\n",
        "    if num_labels > 1:\n",
        "        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "        binary_mask = (labels == largest_label).astype(np.uint8)\n",
        "\n",
        "    # Resize mask to image size\n",
        "    img_np = np.array(img.resize((224,224))).astype(np.float32)/255.0\n",
        "    if img_np.ndim == 2:\n",
        "        img_np = np.stack([img_np]*3, axis=-1)\n",
        "    binary_mask_resized = resize(binary_mask, img_np.shape[:2], order=0, preserve_range=True).astype(np.uint8)\n",
        "\n",
        "    # Overlay tumor in red\n",
        "    overlay = img_np.copy()\n",
        "    overlay[...,0] = np.where(binary_mask_resized==1, 1, overlay[...,0])  # Red channel\n",
        "    overlay[...,1] = np.where(binary_mask_resized==1, 0, overlay[...,1])\n",
        "    overlay[...,2] = np.where(binary_mask_resized==1, 0, overlay[...,2])\n",
        "\n",
        "    return (overlay*255).astype(np.uint8)\n",
        "\n",
        "# Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=highlight_tumor_focused,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
        "        gr.Slider(minimum=0.5, maximum=0.95, step=0.01, value=0.75, label=\"CAM Threshold\")\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"numpy\", label=\"Tumor Highlighted Overlay\"),\n",
        "    title=\"Focused Tumor Highlighting with Hugging Face ViT\",\n",
        "    description=\"Upload an MRI image. The model will highlight the tumor area precisely in red.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2NZxHT4hIUL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM, EigenCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "\n",
        "# --- 1ï¸âƒ£ Load your model ---\n",
        "# wrapped_model = HuggingFaceViTWrapper(...)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "\n",
        "# --- 2ï¸âƒ£ Preprocess input ---\n",
        "def preprocess_image(img):\n",
        "    img = img.resize((224,224))\n",
        "    img_np = np.array(img).astype(np.float32)/255.0\n",
        "    if img_np.ndim == 2:\n",
        "        img_np = np.stack([img_np]*3, axis=-1)\n",
        "    tensor = torch.tensor(img_np).permute(2,0,1).unsqueeze(0).float().to(device)\n",
        "    return tensor, img_np\n",
        "\n",
        "# --- 3ï¸âƒ£ Tumor overlay function ---\n",
        "def highlight_tumor_gradio(img, threshold=0.45):\n",
        "    input_tensor, rgb_image = preprocess_image(img)\n",
        "\n",
        "    # Predict class\n",
        "    with torch.no_grad():\n",
        "        outputs = wrapped_model(input_tensor)\n",
        "    pred_class = outputs.argmax(dim=1).item()\n",
        "    pred_label = wrapped_model.model.config.id2label[pred_class]\n",
        "\n",
        "    # Target layer\n",
        "    target_layer = wrapped_model.model.vit.embeddings.patch_embeddings.projection\n",
        "    targets = [ClassifierOutputTarget(pred_class)]\n",
        "\n",
        "    # GradCAM + EigenCAM\n",
        "    gradcam = GradCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "    eigencam = EigenCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "\n",
        "    grayscale_grad = gradcam(input_tensor=input_tensor, targets=targets)[0]\n",
        "    grayscale_eigen = eigencam(input_tensor=input_tensor, targets=targets)[0]\n",
        "\n",
        "    # Average & normalize\n",
        "    grayscale_cam = (grayscale_grad + grayscale_eigen)/2\n",
        "    grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min() + 1e-8)\n",
        "\n",
        "    # Threshold + Morphology\n",
        "    mask = (grayscale_cam >= threshold).astype('uint8')*255\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((5,5),np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((7,7),np.uint8))\n",
        "\n",
        "    # Largest contour\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    mask_clean = np.zeros_like(mask)\n",
        "    bbox = None\n",
        "    if contours:\n",
        "        largest = max(contours, key=cv2.contourArea)\n",
        "        cv2.drawContours(mask_clean, [largest], -1, 255, -1)\n",
        "        x,y,w,h = cv2.boundingRect(largest)\n",
        "        bbox = (x,y,w,h)\n",
        "\n",
        "    # Overlay\n",
        "    cam_image = show_cam_on_image(rgb_image, mask_clean/255.0, use_rgb=True)\n",
        "\n",
        "    return cam_image, mask_clean, bbox, pred_label\n",
        "\n",
        "# --- 4ï¸âƒ£ Gradio interface ---\n",
        "iface = gr.Interface(\n",
        "    fn=highlight_tumor_gradio,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
        "        gr.Slider(minimum=0.1, maximum=0.9, step=0.01, value=0.45, label=\"CAM Threshold\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(type=\"numpy\", label=\"Tumor Overlay\"),\n",
        "        gr.Image(type=\"numpy\", label=\"Tumor Mask\"),\n",
        "        gr.Textbox(label=\"Bounding Box (x,y,w,h)\"),\n",
        "        gr.Textbox(label=\"Predicted Class\")\n",
        "    ],\n",
        "    title=\"Accurate Tumor Highlighting (GradCAM + EigenCAM)\",\n",
        "    description=\"Upload an MRI image to highlight the tumor area precisely.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC_gdigChm-a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from pytorch_grad_cam import GradCAM, EigenCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "\n",
        "# --- 1ï¸âƒ£ Load your model ---\n",
        "# wrapped_model = HuggingFaceViTWrapper(...)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapped_model.to(device)\n",
        "wrapped_model.eval()\n",
        "\n",
        "# --- 2ï¸âƒ£ Preprocess input ---\n",
        "def preprocess_image(img):\n",
        "    img = img.resize((224,224))\n",
        "    img_np = np.array(img).astype(np.float32)/255.0\n",
        "    if img_np.ndim == 2:\n",
        "        img_np = np.stack([img_np]*3, axis=-1)\n",
        "    tensor = torch.tensor(img_np).permute(2,0,1).unsqueeze(0).float().to(device)\n",
        "    return tensor, img_np\n",
        "\n",
        "# --- 3ï¸âƒ£ Tumor overlay function ---\n",
        "def highlight_tumor_gradio(img, threshold=0.45):\n",
        "    input_tensor, rgb_image = preprocess_image(img)\n",
        "\n",
        "    # Predict class\n",
        "    with torch.no_grad():\n",
        "        outputs = wrapped_model(input_tensor)\n",
        "    pred_class = outputs.argmax(dim=1).item()\n",
        "    pred_label = wrapped_model.model.config.id2label[pred_class].lower()\n",
        "\n",
        "    # --- If predicted class is \"notumor\", skip CAM ---\n",
        "    if pred_label == \"no_tumor\":\n",
        "        mask_clean = np.zeros((224,224), dtype=np.uint8)\n",
        "        cam_image = np.array(rgb_image*255, dtype=np.uint8)\n",
        "        bbox = None\n",
        "        return cam_image, mask_clean, bbox, pred_label\n",
        "\n",
        "    # --- Otherwise, generate CAM for tumor ---\n",
        "    target_layer = wrapped_model.model.vit.embeddings.patch_embeddings.projection\n",
        "    targets = [ClassifierOutputTarget(pred_class)]\n",
        "\n",
        "    gradcam = GradCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "    eigencam = EigenCAM(model=wrapped_model, target_layers=[target_layer])\n",
        "\n",
        "    grayscale_grad = gradcam(input_tensor=input_tensor, targets=targets)[0]\n",
        "    grayscale_eigen = eigencam(input_tensor=input_tensor, targets=targets)[0]\n",
        "\n",
        "    # Average & normalize\n",
        "    grayscale_cam = (grayscale_grad + grayscale_eigen)/2\n",
        "    grayscale_cam = (grayscale_cam - grayscale_cam.min()) / (grayscale_cam.max() - grayscale_cam.min() + 1e-8)\n",
        "\n",
        "    # Threshold + Morphology\n",
        "    mask = (grayscale_cam >= threshold).astype('uint8')*255\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((5,5),np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((7,7),np.uint8))\n",
        "\n",
        "    # Find contours and keep largest tumor region\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    mask_clean = np.zeros_like(mask)\n",
        "    cam_image = np.array(rgb_image*255, dtype=np.uint8)\n",
        "    bbox = None\n",
        "    height, width = mask_clean.shape\n",
        "\n",
        "    if contours:\n",
        "        largest = max(contours, key=cv2.contourArea)\n",
        "        if cv2.contourArea(largest) > 20:  # ignore tiny noise\n",
        "            x, y, w, h = cv2.boundingRect(largest)\n",
        "            # Clip bbox to image size\n",
        "            x = max(x, 0)\n",
        "            y = max(y, 0)\n",
        "            w = min(w, width - x)\n",
        "            h = min(h, height - y)\n",
        "\n",
        "            # Mask CAM strictly inside tumor\n",
        "            safe_cam = np.zeros_like(grayscale_cam)\n",
        "            safe_cam[y:y+h, x:x+w] = grayscale_cam[y:y+h, x:x+w] * (mask[y:y+h, x:x+w]/255.0)\n",
        "\n",
        "            # Update mask_clean for visualization\n",
        "            mask_clean[y:y+h, x:x+w] = mask[y:y+h, x:x+w]\n",
        "\n",
        "            bbox = (x, y, w, h)\n",
        "            cam_image = show_cam_on_image(rgb_image, safe_cam, use_rgb=True)\n",
        "\n",
        "    return cam_image, mask_clean, bbox, pred_label\n",
        "\n",
        "# --- 4ï¸âƒ£ Gradio interface ---\n",
        "iface = gr.Interface(\n",
        "    fn=highlight_tumor_gradio,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
        "        gr.Slider(minimum=0.1, maximum=0.9, step=0.01, value=0.45, label=\"CAM Threshold\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(type=\"numpy\", label=\"Tumor Overlay\"),\n",
        "        gr.Image(type=\"numpy\", label=\"Tumor Mask\"),\n",
        "        gr.Textbox(label=\"Bounding Box (x,y,w,h)\"),\n",
        "        gr.Textbox(label=\"Predicted Class\")\n",
        "    ],\n",
        "    title=\"Tumor Highlighting (GradCAM + EigenCAM)\",\n",
        "    description=\"Upload an MRI image. Heatmap is generated only for glioma, meningioma, or pituitary tumors. Overlay is strictly within tumor boundaries.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Af2L2mQu6C9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# -------------------------------\n",
        "# 1ï¸âƒ£ Dummy U-Net (simulated tumor mask)\n",
        "# -------------------------------\n",
        "class DummyUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, x):\n",
        "        # x: [B,3,H,W]\n",
        "        B, C, H, W = x.shape\n",
        "        mask = torch.zeros((B, 1, H, W))\n",
        "        # Simulate a circular tumor in the center\n",
        "        for b in range(B):\n",
        "            rr, cc = np.ogrid[:H, :W]\n",
        "            center = (H//2, W//2)\n",
        "            radius = H//6\n",
        "            circle = (rr - center[0])**2 + (cc - center[1])**2 <= radius**2\n",
        "            mask[b,0,:,:] = torch.from_numpy(circle.astype(np.float32))\n",
        "        return mask\n",
        "\n",
        "# Instantiate model\n",
        "model = DummyUNet()\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# -------------------------------\n",
        "# 2ï¸âƒ£ Preprocess image\n",
        "# -------------------------------\n",
        "def preprocess_image(img):\n",
        "    img = img.resize((224,224))\n",
        "    img_np = np.array(img).astype(np.float32)/255.0\n",
        "    if img_np.ndim == 2:\n",
        "        img_np = np.stack([img_np]*3, axis=-1)\n",
        "    tensor = torch.tensor(img_np).permute(2,0,1).unsqueeze(0).float().to(device)\n",
        "    return tensor, img_np\n",
        "\n",
        "# -------------------------------\n",
        "# 3ï¸âƒ£ Tumor segmentation & overlay\n",
        "# -------------------------------\n",
        "def segment_tumor_demo(img, threshold=0.5):\n",
        "    input_tensor, rgb_image = preprocess_image(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)  # [1,1,H,W]\n",
        "\n",
        "    mask_prob = output.squeeze(0).squeeze(0).cpu().numpy()\n",
        "    mask_bin = (mask_prob >= threshold).astype(np.uint8) * 255\n",
        "\n",
        "    # Morphology to clean\n",
        "    mask_bin = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
        "    mask_bin = cv2.morphologyEx(mask_bin, cv2.MORPH_CLOSE, np.ones((5,5),np.uint8))\n",
        "\n",
        "    # Overlay tumor mask on original image\n",
        "    cam_image = (rgb_image*255).astype(np.uint8)\n",
        "    color_mask = np.zeros_like(cam_image)\n",
        "    color_mask[:,:,0] = mask_bin  # Red mask\n",
        "    cam_image = cv2.addWeighted(cam_image, 1.0, color_mask, 0.5, 0)\n",
        "\n",
        "    # Bounding box\n",
        "    contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    bbox = None\n",
        "    if contours:\n",
        "        largest = max(contours, key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(largest)\n",
        "        bbox = (x, y, w, h)\n",
        "        cv2.rectangle(cam_image, (x,y), (x+w, y+h), (0,255,0), 2)\n",
        "\n",
        "    return cam_image, mask_bin, bbox\n",
        "\n",
        "# -------------------------------\n",
        "# 4ï¸âƒ£ Gradio interface\n",
        "# -------------------------------\n",
        "iface = gr.Interface(\n",
        "    fn=segment_tumor_demo,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload MRI Image\"),\n",
        "        gr.Slider(minimum=0.1, maximum=0.9, step=0.01, value=0.5, label=\"Mask Threshold\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(type=\"numpy\", label=\"Tumor Overlay\"),\n",
        "        gr.Image(type=\"numpy\", label=\"Tumor Mask\"),\n",
        "        gr.Textbox(label=\"Bounding Box (x,y,w,h)\")\n",
        "    ],\n",
        "    title=\"Tumor Segmentation Demo\",\n",
        "    description=\"This is a demo using a dummy U-Net. Replace with a real trained model for actual results.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sde0FxaVxG-n"
      },
      "outputs": [],
      "source": [
        "pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2no_43Qa0kBQ"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N500_2g80skq"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GhnIX4E21s3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "!cp /content/drive/MyDrive/kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfAGeJwi3E03"
      },
      "outputs": [],
      "source": [
        "zip_path = \"brats20-dataset-training-validation.zip\"\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"{zip_path} already exists âœ…\")\n",
        "else:\n",
        "    # Download dataset from Kaggle\n",
        "    !kaggle datasets download -d awsaf49/brats20-dataset-training-validation\n",
        "    print(f\"{zip_path} downloaded successfully âœ…\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTxq0ArS3nO9"
      },
      "outputs": [],
      "source": [
        "!ls -lh *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYO2hru83pY1"
      },
      "outputs": [],
      "source": [
        "!unzip -q brats20-dataset-training-validation.zip -d ./brats20\n",
        "!ls ./brats20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2jfMfqK52xh"
      },
      "outputs": [],
      "source": [
        "# 1ï¸âƒ£ Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2ï¸âƒ£ Define paths\n",
        "source_path = \"/content/brats20\"  # current location in Colab\n",
        "dest_path = \"/content/drive/MyDrive/BRATS20\"  # destination folder in Drive\n",
        "\n",
        "# 3ï¸âƒ£ Move the folder to Drive\n",
        "import shutil\n",
        "shutil.move(source_path, dest_path)\n",
        "\n",
        "print(f\"Dataset moved to Google Drive at: {dest_path}\")\n",
        "\n",
        "# 4ï¸âƒ£ Verify contents\n",
        "import os\n",
        "print(\"Root contents:\", os.listdir(dest_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3YEKgaRE5aJ"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "temp_folder = \"/content/brats20_temp\"\n",
        "\n",
        "if os.path.exists(temp_folder):\n",
        "    shutil.rmtree(temp_folder)\n",
        "    print(f\"{temp_folder} deleted successfully âœ…\")\n",
        "else:\n",
        "    print(f\"{temp_folder} does not exist âŒ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCi-6ohlLiLF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "drive_folder = \"/content/drive/MyDrive/BRATS20\"\n",
        "print(\"Top-level folders in BRATS20:\")\n",
        "print(os.listdir(drive_folder))\n",
        "\n",
        "# Check a few sample files inside TrainingData\n",
        "train_folder = os.path.join(drive_folder, \"BraTS2020_TrainingData\")\n",
        "if os.path.exists(train_folder):\n",
        "    print(\"\\nSample TrainingData files/folders:\")\n",
        "    print(os.listdir(train_folder)[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbzdlDnWMxxz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "drive_folder = \"/content/drive/MyDrive/BRATS20\"\n",
        "for root, dirs, files in os.walk(drive_folder):\n",
        "    print(root, len(dirs), len(files))  # shows folder, number of subfolders, number of files\n",
        "    break  # only show top-level\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "depatMilO3l7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "drive_folder = \"/content/drive/MyDrive/BRATS20\"\n",
        "\n",
        "print(\"Top-level folders in BRATS20:\")\n",
        "print(os.listdir(drive_folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWniJfHIO4zI"
      },
      "outputs": [],
      "source": [
        "train_folder = os.path.join(drive_folder, \"BraTS2020_TrainingData\", \"MICCAI_BraTS2020_TrainingData\")\n",
        "print(\"\\nSample patient folders in TrainingData:\")\n",
        "print(os.listdir(train_folder)[:5])  # first 5 patients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_NpUAppO8Wy"
      },
      "outputs": [],
      "source": [
        "val_folder = os.path.join(drive_folder, \"BraTS2020_ValidationData\", \"MICCAI_BraTS2020_ValidationData\")\n",
        "print(\"\\nSample patient folders in ValidationData:\")\n",
        "print(os.listdir(val_folder)[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55yO4V2QO-X4"
      },
      "outputs": [],
      "source": [
        "num_train = len(os.listdir(train_folder))\n",
        "num_val = len(os.listdir(val_folder))\n",
        "print(f\"\\nTotal Training patients: {num_train}\")\n",
        "print(f\"Total Validation patients: {num_val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcSFEUuTUNq5"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location=\"cpu\")\n",
        "print(\"Keys:\", list(state_dict.keys())[:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaTBLgMjUXuo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "\n",
        "# Load ViT model with same structure used during training\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=4)  # change num_classes if needed\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brain_tumor_model.pth\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "print(\"âœ… Model loaded successfully and ready for Grad-CAM!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e00KarwjUjZ0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Example test image (replace with your MRI slice path)\n",
        "image_path = \"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"  # you can extract one MRI slice and use it\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=0.5, std=0.5),\n",
        "])\n",
        "\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n",
        "print(\"Model output:\", outputs)\n",
        "print(\"Predicted class:\", outputs.argmax(dim=1).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3XAZ04-xX1j"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception as e:\n",
        "    print(\"Not running inside Colab or drive mount step failed:\", e)\n",
        "\n",
        "import os, sys\n",
        "dataset_path = '/content/drive/MyDrive/BRATS20'\n",
        "model_path   = '/content/drive/MyDrive/brain_tumor_vit_model'\n",
        "\n",
        "print(\"== PATH CHECK ==\")\n",
        "print(\"Dataset path:\", dataset_path, \"-> exists?\", os.path.exists(dataset_path))\n",
        "print(\"Model path  :\", model_path,   \"-> exists?\", os.path.exists(model_path))\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"\\n-- Dataset folder listing (first 30 entries) --\")\n",
        "    print(os.listdir(dataset_path)[:30])\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"\\n-- Model folder listing (first 50 entries) --\")\n",
        "    model_files = os.listdir(model_path)\n",
        "    print(model_files[:50])\n",
        "else:\n",
        "    model_files = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg50oJo5ybBV"
      },
      "outputs": [],
      "source": [
        " !pip install -q timm grad-cam opencv-python-headless matplotlib nibabel SimpleITK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lneNjzc4wfv"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vit_b_16\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Mt7Ocs43jq"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Define ViT model architecture ---\n",
        "num_classes = 3  # replace with the number of classes in your BraTS model\n",
        "\n",
        "# Create ViT model without pre-trained weights\n",
        "model = vit_b_16(weights=None)\n",
        "\n",
        "# Modify the classification head to match number of classes\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "\n",
        "# Move model to device and set to evaluation mode\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"ViT model loaded and ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dZXHwqY482e"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "\n",
        "# Create model with 4 classes (matching checkpoint)\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4)\n",
        "\n",
        "# Load checkpoint\n",
        "state_dict = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Move to device and eval\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model backbone loaded. Head matches checkpoint.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B70yq4hF5WBI"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# --- Path to NIfTI file ---\n",
        "nii_path = \"/content/drive/MyDrive/BRATS20/BraTS2020_ValidationData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_024/BraTS20_Training_024_flair.nii\"\n",
        "\n",
        "# --- Load NIfTI ---\n",
        "nii_img = nib.load(nii_path)\n",
        "img_data = nii_img.get_fdata()  # shape: (H, W, D) or (X,Y,Z)\n",
        "\n",
        "# --- Take a middle slice along axial plane (or choose slice you want) ---\n",
        "slice_idx = img_data.shape[2] // 2\n",
        "slice_img = img_data[:, :, slice_idx]\n",
        "\n",
        "# --- Normalize to 0-255 ---\n",
        "slice_img = slice_img - slice_img.min()\n",
        "slice_img = slice_img / slice_img.max()\n",
        "slice_img = (slice_img * 255).astype(np.uint8)\n",
        "\n",
        "# --- Convert to 3-channel RGB (ViT expects 3 channels) ---\n",
        "slice_rgb = np.stack([slice_img]*3, axis=-1)  # shape: (H,W,3)\n",
        "slice_rgb = Image.fromarray(slice_rgb)\n",
        "\n",
        "# --- Preprocess for ViT ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "input_tensor = transform(slice_rgb).unsqueeze(0).to(device)\n",
        "print(\"NIfTI slice loaded and preprocessed. Shape:\", input_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXVn68Wu5WH8"
      },
      "outputs": [],
      "source": [
        "# --- Step 5: Predict class ---\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n",
        "    pred_class = outputs.argmax(dim=1).item()\n",
        "\n",
        "print(\"Predicted class index:\", pred_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p03Co3sk59Dy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pytorch_grad_cam import LayerCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "# --- Load your slice ---\n",
        "slice_path = \"/content/sample_brain_slice.png\"  # replace with your slice path\n",
        "slice_rgb = Image.open(slice_path).convert(\"RGB\")\n",
        "rgb_img = np.array(slice_rgb.resize((224, 224))) / 255.0  # normalize 0-1\n",
        "\n",
        "# --- Prepare input tensor ---\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(slice_rgb).unsqueeze(0)  # [1,3,224,224]\n",
        "\n",
        "# --- Predict class ---\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n",
        "pred_class = outputs.argmax(dim=1).item()\n",
        "\n",
        "# --- Use LayerCAM for smoother visualization ---\n",
        "target_layers = [model.blocks[-1].norm2]  # last ViT block\n",
        "cam = LayerCAM(model=model, target_layers=target_layers)\n",
        "targets = [ClassifierOutputTarget(pred_class)]\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
        "\n",
        "# --- Resize CAM to full image and normalize ---\n",
        "grayscale_cam_resized = F.interpolate(\n",
        "    torch.tensor(grayscale_cam[None, None, :, :]),\n",
        "    size=(224, 224),\n",
        "    mode='bilinear',\n",
        "    align_corners=False\n",
        ")[0,0].numpy()\n",
        "\n",
        "# Normalize CAM to 0-1\n",
        "grayscale_cam_resized = (grayscale_cam_resized - grayscale_cam_resized.min()) / (grayscale_cam_resized.max() - grayscale_cam_resized.min())\n",
        "\n",
        "# Apply Gaussian blur to remove stripes\n",
        "grayscale_cam_smooth = cv2.GaussianBlur(grayscale_cam_resized, (11, 11), 0)\n",
        "\n",
        "# --- Overlay CAM on original image ---\n",
        "visualization = show_cam_on_image(rgb_img, grayscale_cam_smooth, use_rgb=True, colormap=cv2.COLORMAP_JET)\n",
        "\n",
        "# --- Display ---\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(visualization)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQF2faOrQsKi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# ------------------------\n",
        "# 1ï¸âƒ£ Define a simple U-Net\n",
        "# ------------------------\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64,128,256,512]):\n",
        "        super(UNet, self).__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Encoder\n",
        "        for feat in features:\n",
        "            self.downs.append(nn.Sequential(\n",
        "                nn.Conv2d(in_channels, feat, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(feat, feat, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ))\n",
        "            in_channels = feat\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Decoder\n",
        "        for feat in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feat*2, feat, kernel_size=2, stride=2))\n",
        "            self.ups.append(nn.Sequential(\n",
        "                nn.Conv2d(feat*2, feat, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(feat, feat, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ))\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features[-1], features[-1]*2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(features[-1]*2, features[-1]*2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip = skip_connections[idx//2]\n",
        "            if x.shape != skip.shape:\n",
        "                x = F.interpolate(x, size=skip.shape[2:])\n",
        "            x = torch.cat([skip, x], dim=1)\n",
        "            x = self.ups[idx+1](x)\n",
        "        return torch.sigmoid(self.final_conv(x))  # output 0-1 mask\n",
        "\n",
        "# ------------------------\n",
        "# 2ï¸âƒ£ Load slice\n",
        "# ------------------------\n",
        "slice_path = \"/content/drive/MyDrive/test_images/sample-img10-meningioma.jpg\"  # replace with your slice path\n",
        "slice_img = Image.open(slice_path).convert(\"RGB\")\n",
        "\n",
        "# Preprocess\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "])\n",
        "input_tensor = preprocess(slice_img).unsqueeze(0)  # [1,3,224,224]\n",
        "\n",
        "# ------------------------\n",
        "# 3ï¸âƒ£ Load trained U-Net\n",
        "# ------------------------\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "# If you have trained weights:\n",
        "# model.load_state_dict(torch.load(\"unet_brats.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# ------------------------\n",
        "# 4ï¸âƒ£ Predict mask\n",
        "# ------------------------\n",
        "with torch.no_grad():\n",
        "    mask_pred = model(input_tensor)[0,0].cpu().numpy()  # [224,224]\n",
        "\n",
        "# Smooth mask\n",
        "mask_pred_smooth = cv2.GaussianBlur(mask_pred, (11,11), 0)\n",
        "\n",
        "# Normalize\n",
        "mask_pred_smooth = (mask_pred_smooth - mask_pred_smooth.min()) / (mask_pred_smooth.max() - mask_pred_smooth.min())\n",
        "\n",
        "# ------------------------\n",
        "# 5ï¸âƒ£ Overlay on original image\n",
        "# ------------------------\n",
        "rgb_img = np.array(slice_img.resize((224,224))) / 255.0\n",
        "overlay = rgb_img.copy()\n",
        "overlay[:,:,0] = np.where(mask_pred_smooth>0.5, 1.0, overlay[:,:,0])  # red highlight\n",
        "\n",
        "visualization = cv2.addWeighted(rgb_img, 0.7, overlay, 0.3, 0)\n",
        "\n",
        "# ------------------------\n",
        "# 6ï¸âƒ£ Display\n",
        "# ------------------------\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(visualization)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg7FiiwykY5x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhCzt6dDkaII"
      },
      "outputs": [],
      "source": [
        "class BratsDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # grayscale mask\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        mask = np.array(mask)/255.0  # normalize to 0-1\n",
        "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pJRSIB2keuW"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrjFBbvZkgfI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "brats_path = \"/content/drive/MyDrive/BRATS20\"  # replace with your path\n",
        "\n",
        "# List all subfolders and files\n",
        "for root, dirs, files in os.walk(brats_path):\n",
        "    print(f\"Folder: {root}\")\n",
        "    print(f\"  Subfolders: {dirs}\")\n",
        "    print(f\"  Number of files: {len(files)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-a-O3UZledW"
      },
      "outputs": [],
      "source": [
        "for root, dirs, files in os.walk(brats_path, topdown=False):\n",
        "    for d in dirs:\n",
        "        dir_path = os.path.join(root, d)\n",
        "        if len(os.listdir(dir_path)) == 0:\n",
        "            print(f\"Removing empty folder: {dir_path}\")\n",
        "            os.rmdir(dir_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABQU1bm4lnS4"
      },
      "outputs": [],
      "source": [
        "valid_ext = (\".png\", \".jpg\", \".jpeg\", \".nii\", \".nii.gz\")  # adjust for BRATS\n",
        "for root, dirs, files in os.walk(brats_path):\n",
        "    for f in files:\n",
        "        if not f.lower().endswith(valid_ext):\n",
        "            print(f\"Found non-image file: {os.path.join(root, f)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC7BjnVilrto"
      },
      "outputs": [],
      "source": [
        "for root, dirs, files in os.walk(brats_path):\n",
        "    for f in files:\n",
        "        if not f.lower().endswith(valid_ext):\n",
        "            os.remove(os.path.join(root, f))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZn_oon5lxKK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/BRATS20\"\n",
        "images_out = os.path.join(base_path, \"images\")\n",
        "masks_out = os.path.join(base_path, \"masks\")\n",
        "\n",
        "os.makedirs(images_out, exist_ok=True)\n",
        "os.makedirs(masks_out, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmlD9lsXnYDy"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Paths to Training and Validation folders\n",
        "data_folders = [\n",
        "    os.path.join(base_path, \"BraTS2020_ValidationData\", \"MICCAI_BraTS2020_TrainingData\"),\n",
        "    os.path.join(base_path, \"BraTS2020_ValidationData\", \"MICCAI_BraTS2020_ValidationData\")\n",
        "]\n",
        "\n",
        "for data_folder in data_folders:\n",
        "    patient_dirs = [f for f in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, f))]\n",
        "\n",
        "    for patient in patient_dirs:\n",
        "        patient_path = os.path.join(data_folder, patient)\n",
        "\n",
        "        # List all files in patient folder\n",
        "        files = os.listdir(patient_path)\n",
        "\n",
        "        # Identify MRI modality (choose one, e.g., FLAIR)\n",
        "        flair_files = [f for f in files if \"flair.nii\" in f.lower()]\n",
        "        seg_files   = [f for f in files if \"seg.nii\" in f.lower()]\n",
        "\n",
        "        if len(flair_files) == 0:\n",
        "            continue  # skip if no flair\n",
        "\n",
        "        flair_path = os.path.join(patient_path, flair_files[0])\n",
        "        mask_path  = os.path.join(patient_path, seg_files[0]) if seg_files else None\n",
        "\n",
        "        # Load NIfTI\n",
        "        img_nii = nib.load(flair_path)\n",
        "        img_data = img_nii.get_fdata()\n",
        "\n",
        "        mask_data = None\n",
        "        if mask_path:\n",
        "            mask_nii = nib.load(mask_path)\n",
        "            mask_data = mask_nii.get_fdata()\n",
        "\n",
        "        # Iterate over axial slices\n",
        "        for i in range(img_data.shape[2]):\n",
        "            img_slice = img_data[:, :, i]\n",
        "\n",
        "            if mask_data is not None:\n",
        "                mask_slice = mask_data[:, :, i]\n",
        "                if np.max(mask_slice) == 0:\n",
        "                    continue  # skip empty slices\n",
        "            else:\n",
        "                mask_slice = None  # validation data has no masks\n",
        "\n",
        "            # Normalize image slice\n",
        "            img_norm = ((img_slice - np.min(img_slice)) / (np.max(img_slice) - np.min(img_slice)) * 255).astype(np.uint8)\n",
        "            img_name = f\"{patient}_slice_{i}.png\"\n",
        "            Image.fromarray(img_norm).save(os.path.join(images_out, img_name))\n",
        "\n",
        "            # Save mask if exists\n",
        "            if mask_slice is not None:\n",
        "                mask_norm = (mask_slice * 255 / mask_slice.max()).astype(np.uint8)\n",
        "                mask_name = f\"{patient}_slice_{i}.png\"\n",
        "                Image.fromarray(mask_norm).save(os.path.join(masks_out, mask_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQvknkEKtHnc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/BRATS20\"\n",
        "images_out = os.path.join(base_path, \"images\")\n",
        "masks_out  = os.path.join(base_path, \"masks\")\n",
        "\n",
        "# List images\n",
        "image_files = sorted([f for f in os.listdir(images_out) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n",
        "print(f\"Total image slices: {len(image_files)}\")\n",
        "print(\"First 10 image files:\", image_files[:10])\n",
        "\n",
        "# List masks\n",
        "mask_files = sorted([f for f in os.listdir(masks_out) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n",
        "print(f\"Total mask slices: {len(mask_files)}\")\n",
        "print(\"First 10 mask files:\", mask_files[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I83kcOStSKD"
      },
      "outputs": [],
      "source": [
        "# Create list of images that have masks\n",
        "masked_images = [f for f in image_files if f in mask_files]\n",
        "print(f\"Total slices usable for training: {len(masked_images)}\")\n",
        "print(\"First 10 usable slices:\", masked_images[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaZK6l0GtinN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class BratsSliceDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "\n",
        "        # Only keep images that have corresponding masks\n",
        "        image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(\".png\")]\n",
        "        mask_files  = [f for f in os.listdir(masks_dir) if f.lower().endswith(\".png\")]\n",
        "        self.files = sorted([f for f in image_files if f in mask_files])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.files[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        mask_path = os.path.join(self.masks_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        # Ensure mask is 0 or 1\n",
        "        mask = torch.tensor(np.array(mask)/255., dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd_HMjNKtkRh"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transform: resize, to tensor, normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "# Initialize Dataset\n",
        "dataset = BratsSliceDataset(images_dir=images_out, masks_dir=masks_out, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "print(f\"Total training slices: {len(dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moBFT6KMtuVV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64,128,256,512]):\n",
        "        super(UNet, self).__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Downsampling path\n",
        "        for feature in features:\n",
        "            self.downs.append(self.double_conv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.double_conv(features[-1], features[-1]*2)\n",
        "\n",
        "        # Upsampling path\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.ups.append(self.double_conv(feature*2, feature))\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            x = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](x)\n",
        "\n",
        "        return torch.sigmoid(self.final_conv(x))\n",
        "\n",
        "    def double_conv(self, in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_0OrJo5t38Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Instantiate model\n",
        "model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()  # binary segmentation\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDwPw6eqt_hn"
      },
      "outputs": [],
      "source": [
        "class BratsSliceDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "\n",
        "        image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(\".png\")]\n",
        "        mask_files  = [f for f in os.listdir(masks_dir) if f.lower().endswith(\".png\")]\n",
        "        self.files = sorted([f for f in image_files if f in mask_files])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.files[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        mask_path = os.path.join(self.masks_dir, img_name)\n",
        "\n",
        "        # Load grayscale\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # Resize both to 224x224\n",
        "        image = image.resize((224,224))\n",
        "        mask = mask.resize((224,224))\n",
        "\n",
        "        # Apply transform only to image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert mask to tensor manually\n",
        "        mask = torch.tensor(np.array(mask)/255., dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Check if model expects grayscale or RGB\n",
        "expect_rgb = False  # change to True if model pretrained on RGB images\n",
        "\n",
        "if expect_rgb:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=3),  # ensure 3 channels\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "else:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  # for grayscale\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "cafWeX8vH5Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = BratsSliceDataset(images_dir=images_out, masks_dir=masks_out, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "qVPik8eWH8QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "KdIr_9FCITXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY6M76uovLk9"
      },
      "outputs": [],
      "source": [
        "dataset = BratsSliceDataset(images_dir=images_out, masks_dir=masks_out, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "images, masks = next(iter(loader))\n",
        "print(images.shape, masks.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSfffkwVvd5n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Model (already defined)\n",
        "model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()  # binary segmentation\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 50\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, masks in loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Save model weights to your Drive ---\n",
        "save_path = \"/content/drive/MyDrive/brats_trained_model.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"âœ… Model saved successfully at: {save_path}\")\n"
      ],
      "metadata": {
        "id": "R1WlQb59gTPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create model structure\n",
        "model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "\n",
        "# Load saved weights\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"âœ… Model loaded and ready for inference\")\n"
      ],
      "metadata": {
        "id": "AwZLiQStgd-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coefficient(preds, targets, smooth=1e-6):\n",
        "    preds = (preds > 0.5).float()  # threshold\n",
        "    intersection = (preds * targets).sum(dim=(1,2,3))\n",
        "    union = preds.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3))\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "    return dice.mean()\n",
        "\n",
        "# --- After training loop ---\n",
        "model.eval()\n",
        "dice_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in loader:  # or use a separate val_loader if available\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = torch.sigmoid(model(images))\n",
        "        dice = dice_coefficient(outputs, masks)\n",
        "        dice_scores.append(dice.item())\n",
        "\n",
        "print(f\"âœ… Mean Dice score: {sum(dice_scores)/len(dice_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "uDjRPwJcRjtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in [0.2, 0.3, 0.5]:\n",
        "    score = []\n",
        "    for images, masks in loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = torch.sigmoid(model(images))\n",
        "        preds = (outputs > t).float()\n",
        "        intersection = (preds * masks).sum(dim=(1,2,3))\n",
        "        union = preds.sum(dim=(1,2,3)) + masks.sum(dim=(1,2,3))\n",
        "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
        "        score.append(dice.mean().item())\n",
        "    print(f\"Threshold {t}: Mean Dice {sum(score)/len(score):.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SBXvqrGpR7Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img, mask = dataset[10]\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "plt.title(\"Image\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(mask[0], cmap='gray')\n",
        "plt.title(f\"Mask (min={mask.min():.1f}, max={mask.max():.1f})\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1zTh0jjlSZxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load model ---\n",
        "model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded successfully.\")\n",
        "\n",
        "# --- Preprocessing transform (same as training) ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Load a test image ---\n",
        "img_path = \"/content/drive/MyDrive/test_images/sample-img9-glioma.jpg\"   # ðŸ‘ˆ change to your image path\n",
        "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "img_resized = cv2.resize(img, (224, 224))\n",
        "\n",
        "# --- Prepare tensor ---\n",
        "input_tensor = transform(img_resized).unsqueeze(0).to(device)  # [1,1,224,224]\n",
        "\n",
        "# --- Predict mask ---\n",
        "with torch.no_grad():\n",
        "    pred_mask = torch.sigmoid(model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "# --- Normalize prediction ---\n",
        "pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "\n",
        "# --- Create heatmap ---\n",
        "heatmap = cv2.applyColorMap(np.uint8(255 * pred_mask), cv2.COLORMAP_JET)\n",
        "\n",
        "# --- Overlay heatmap on original grayscale image ---\n",
        "overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "# --- Display results ---\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img_resized, cmap='gray')\n",
        "plt.title(\"Original MRI\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(pred_mask, cmap='jet')\n",
        "plt.title(\"Predicted Mask\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Heatmap Overlay\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7gy1xGrog64L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNet, self).__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        # Down path\n",
        "        for feature in features:\n",
        "            self.downs.append(self.double_conv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.double_conv(features[-1], features[-1]*2)\n",
        "\n",
        "        # Up path\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(self.double_conv(feature*2, feature))\n",
        "\n",
        "        # Final conv\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def double_conv(self, in_c, out_c):\n",
        "        # âœ… bias=True so weights match your checkpoint\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip = skip_connections[idx//2]\n",
        "            if x.shape != skip.shape:\n",
        "                x = TF.resize(x, size=skip.shape[2:])\n",
        "            x = torch.cat((skip, x), dim=1)\n",
        "            x = self.ups[idx+1](x)\n",
        "\n",
        "        return torch.sigmoid(self.final_conv(x))\n"
      ],
      "metadata": {
        "id": "pCSKDkMO0WOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(in_channels=1, out_channels=1)\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location='cpu')\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded successfully with bias=True!\")\n"
      ],
      "metadata": {
        "id": "h-muAb8Z00fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optional: Threshold the mask for binary segmentation ---\n",
        "threshold = 0.5\n",
        "binary_mask = (pred_mask > threshold).astype(np.uint8) * 255  # 0 or 255\n",
        "\n",
        "# --- Save results ---\n",
        "output_dir = \"/content/drive/MyDrive/results/\"\n",
        "cv2.imwrite(output_dir + \"pred_mask.png\", np.uint8(pred_mask * 255))\n",
        "cv2.imwrite(output_dir + \"binary_mask.png\", binary_mask)\n",
        "cv2.imwrite(output_dir + \"overlay.png\", overlay)\n",
        "print(\"âœ… Results saved to\", output_dir)\n",
        "\n",
        "# --- Optional: Extract contours from the binary mask ---\n",
        "contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "img_contours = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR)\n",
        "cv2.drawContours(img_contours, contours, -1, (0,255,0), 2)\n",
        "\n",
        "# --- Display contours overlay ---\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cv2.cvtColor(img_contours, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Contours Overlay\")\n",
        "plt.show()\n",
        "\n",
        "# --- Optional: Compute simple metrics if ground truth available ---\n",
        "# gt_mask = cv2.imread(\"/path/to/ground_truth.png\", cv2.IMREAD_GRAYSCALE)\n",
        "# dice_score = 2 * np.sum(binary_mask & gt_mask) / (np.sum(binary_mask) + np.sum(gt_mask))\n",
        "# print(\"Dice Score:\", dice_score)\n"
      ],
      "metadata": {
        "id": "VxLBrRlm1Mgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Normalize prediction for heatmap ---\n",
        "pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "\n",
        "# --- Create heatmap (colored) ---\n",
        "heatmap = cv2.applyColorMap(np.uint8(255 * pred_mask), cv2.COLORMAP_JET)\n",
        "\n",
        "# --- Overlay heatmap on original grayscale image ---\n",
        "alpha = 0.4  # transparency for heatmap\n",
        "overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 1-alpha, heatmap, alpha, 0)\n",
        "\n",
        "# --- Display results ---\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img_resized, cmap='gray')\n",
        "plt.title(\"Original MRI\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(heatmap[..., ::-1])  # Convert BGR to RGB\n",
        "plt.title(\"Heatmap\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(overlay[..., ::-1])\n",
        "plt.title(\"Heatmap Overlay\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# --- Save heatmap results ---\n",
        "output_dir = \"/content/drive/MyDrive/results/\"\n",
        "cv2.imwrite(output_dir + \"heatmap.png\", heatmap)\n",
        "cv2.imwrite(output_dir + \"overlay_heatmap.png\", overlay)\n",
        "print(\"âœ… Heatmap and overlay saved to\", output_dir)\n"
      ],
      "metadata": {
        "id": "j52hz3LO1nfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Example: UNet prediction mask and original image ---\n",
        "# pred_mask: predicted mask from UNet (float32, range 0-1)\n",
        "# img: original grayscale MRI image\n",
        "\n",
        "# --- Step 1: Resize image exactly as training ---\n",
        "img_resized = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "pred_mask_resized = cv2.resize(pred_mask, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# --- Step 2: Smooth prediction to reduce noise ---\n",
        "pred_mask_smooth = cv2.GaussianBlur(pred_mask_resized, (5,5), 0)\n",
        "\n",
        "# --- Step 3: Clip low-confidence areas and normalize ---\n",
        "pred_mask_clipped = np.clip(pred_mask_smooth, 0.2, 1.0)  # focus on >20% confidence\n",
        "pred_mask_norm = (pred_mask_clipped - pred_mask_clipped.min()) / (pred_mask_clipped.max() - pred_mask_clipped.min())\n",
        "\n",
        "# --- Step 4: Create heatmap ---\n",
        "heatmap = cv2.applyColorMap(np.uint8(255 * pred_mask_norm), cv2.COLORMAP_JET)\n",
        "\n",
        "# --- Step 5: Overlay only high-confidence areas ---\n",
        "alpha = 0.5\n",
        "overlay = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR).copy()\n",
        "mask_high = pred_mask_norm > 0.3\n",
        "overlay[mask_high] = cv2.addWeighted(overlay, 1-alpha, heatmap, alpha, 0)[mask_high]\n",
        "\n",
        "# --- Step 6 (Optional): Draw contours for clear tumor boundaries ---\n",
        "binary_mask = (pred_mask_norm > 0.3).astype(np.uint8) * 255\n",
        "contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(overlay, contours, -1, (0,255,0), 2)\n",
        "\n",
        "# --- Step 7: Display results ---\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img_resized, cmap='gray')\n",
        "plt.title(\"Original MRI\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(heatmap[..., ::-1])  # Convert BGR to RGB\n",
        "plt.title(\"Heatmap\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(overlay[..., ::-1])\n",
        "plt.title(\"Heatmap Overlay\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# --- Step 8: Save results ---\n",
        "output_dir = \"/content/drive/MyDrive/results/\"\n",
        "cv2.imwrite(output_dir + \"heatmap.png\", heatmap)\n",
        "cv2.imwrite(output_dir + \"overlay_heatmap.png\", overlay)\n",
        "print(\"âœ… Heatmap and overlay saved to\", output_dir)\n"
      ],
      "metadata": {
        "id": "twFSK1XV2HI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# =========================================================\n",
        "# Step 1 â€” Load original MRI image\n",
        "# =========================================================\n",
        "img_path = \"/content/drive/MyDrive/test_images/sample-img4-pituitary.jpg\"  # change to your image path\n",
        "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# normalize for model input\n",
        "img_input = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "img_input = img_input.astype(np.float32) / 255.0\n",
        "img_tensor = torch.from_numpy(img_input).unsqueeze(0).unsqueeze(0)  # shape: [1,1,224,224]\n",
        "\n",
        "# =========================================================\n",
        "# Step 2 â€” Predict mask using your trained U-Net\n",
        "# =========================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# make sure you have loaded your trained U-Net model\n",
        "# model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "# model.load_state_dict(torch.load(\"/path/to/model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(img_tensor.to(device))\n",
        "    pred_mask = torch.sigmoid(pred).cpu().numpy()[0,0]  # shape (224,224), range 0â€“1\n",
        "\n",
        "# =========================================================\n",
        "# Step 3 â€” Refined Heatmap Visualization\n",
        "# =========================================================\n",
        "img_resized = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "pred_mask_resized = cv2.resize(pred_mask, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Emphasize high-confidence tumor areas\n",
        "pred_mask_enhanced = np.power(pred_mask_resized, 2.5)\n",
        "pred_mask_norm = cv2.normalize(pred_mask_enhanced, None, 0, 1, cv2.NORM_MINMAX)\n",
        "pred_mask_thresh = np.where(pred_mask_norm > 0.4, pred_mask_norm, 0)\n",
        "pred_mask_smooth = cv2.GaussianBlur(pred_mask_thresh, (3,3), 0)\n",
        "\n",
        "# Generate heatmap\n",
        "heatmap = cv2.applyColorMap(np.uint8(255 * pred_mask_smooth), cv2.COLORMAP_JET)\n",
        "\n",
        "# Overlay on grayscale MRI\n",
        "img_color = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR)\n",
        "overlay = cv2.addWeighted(img_color, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "# Optional: draw contours\n",
        "binary_mask = (pred_mask_smooth > 0.4).astype(np.uint8) * 255\n",
        "contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(overlay, contours, -1, (0, 255, 0), 1)\n",
        "\n",
        "# =========================================================\n",
        "# Step 4 â€” Display results\n",
        "# =========================================================\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img_resized, cmap='gray')\n",
        "plt.title(\"Original MRI\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(heatmap[..., ::-1])\n",
        "plt.title(\"Tumor Heatmap\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(overlay[..., ::-1])\n",
        "plt.title(\"Overlay (Precise)\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HzfnC3N-r0HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "import os\n",
        "\n",
        "# --- Device setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load trained model ---\n",
        "model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded successfully.\")\n",
        "\n",
        "# --- Preprocessing transform (same as training) ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Load a test image ---\n",
        "img_path = \"/content/drive/MyDrive/test_images/sample_img1-meningioma.jpg\"  # ðŸ‘ˆ change path if needed\n",
        "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "img_resized = cv2.resize(img, (224, 224))\n",
        "\n",
        "# --- Prepare input tensor ---\n",
        "input_tensor = transform(img_resized).unsqueeze(0).to(device)  # [1,1,224,224]\n",
        "\n",
        "# --- Predict segmentation mask ---\n",
        "with torch.no_grad():\n",
        "    pred_mask = torch.sigmoid(model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "# --- Normalize and smooth prediction ---\n",
        "pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "pred_mask = cv2.GaussianBlur(pred_mask, (3, 3), 0)  # smooth edges\n",
        "\n",
        "# --- Create color heatmap ---\n",
        "heatmap = cv2.applyColorMap(np.uint8(255 * pred_mask), cv2.COLORMAP_JET)\n",
        "\n",
        "# --- Overlay heatmap on grayscale MRI ---\n",
        "overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "# --- Draw tumor contours ---\n",
        "contours, _ = cv2.findContours((pred_mask > 0.4).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(overlay, contours, -1, (0, 255, 0), 1)\n",
        "\n",
        "# --- Create output directory ---\n",
        "output_dir = \"/content/drive/MyDrive/results/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# --- Save outputs ---\n",
        "heatmap_path = os.path.join(output_dir, \"tumor_heatmap.png\")\n",
        "overlay_path = os.path.join(output_dir, \"tumor_heatmap_overlay.png\")\n",
        "cv2.imwrite(heatmap_path, heatmap)\n",
        "cv2.imwrite(overlay_path, overlay)\n",
        "print(f\"âœ… Heatmap and overlay saved to: {output_dir}\")\n",
        "\n",
        "# --- Display results ---\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img_resized, cmap='gray')\n",
        "plt.title(\"Original MRI\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(heatmap[..., ::-1])  # BGRâ†’RGB\n",
        "plt.title(\"Tumor Heatmap\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Overlay with Contours\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lZR_WDs-tWpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count predicted tumor regions\n",
        "binary_mask = (pred_mask > 0.5).astype(np.uint8)\n",
        "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, connectivity=8)\n",
        "print(\"Detected regions:\", num_labels - 1)\n",
        "print(\"Area of each region:\", stats[1:, cv2.CC_STAT_AREA])\n"
      ],
      "metadata": {
        "id": "kImmAN67t5FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.measure import label, regionprops\n",
        "\n",
        "# --- Step 1: Set dataset directories ---\n",
        "# Update these paths to match your dataset folder structure\n",
        "images_dir = \"/content/drive/MyDrive/BRATS20/images\"   # Folder with MRI images\n",
        "masks_dir = \"/content/drive/MyDrive/BRATS20/masks\"     # Folder with segmentation masks\n",
        "\n",
        "# --- Step 2: Get all filenames ---\n",
        "image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "# Sanity check\n",
        "if not image_files or not mask_files:\n",
        "    raise FileNotFoundError(\"âŒ No image or mask files found in the specified directories!\")\n",
        "\n",
        "# --- Step 3: Loop through dataset ---\n",
        "for i, (img_name, mask_name) in enumerate(zip(image_files, mask_files)):\n",
        "    print(f\"\\nðŸ§© Processing Pair {i+1}: {img_name} & {mask_name}\")\n",
        "\n",
        "    # Load image and mask\n",
        "    img = cv2.imread(os.path.join(images_dir, img_name))\n",
        "    mask = cv2.imread(os.path.join(masks_dir, mask_name), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if img is None or mask is None:\n",
        "        print(f\"âš ï¸ Skipping pair {img_name}, {mask_name} due to read error.\")\n",
        "        continue\n",
        "\n",
        "    # --- Step 4: Resize (optional if already 240x240 or 256x256) ---\n",
        "    img_resized = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "    mask_resized = cv2.resize(mask, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # --- Step 5: Threshold mask to binary ---\n",
        "    _, binary_mask = cv2.threshold(mask_resized, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # --- Step 6: Detect connected regions ---\n",
        "    labeled_mask = label(binary_mask)\n",
        "    regions = regionprops(labeled_mask)\n",
        "\n",
        "    num_regions = len(regions)\n",
        "    areas = [int(r.area) for r in regions]\n",
        "\n",
        "    print(f\"âœ… Detected regions: {num_regions}\")\n",
        "    print(f\"ðŸ“ Area of each region: {areas}\")\n",
        "\n",
        "    # --- Step 7: Overlay visualization ---\n",
        "    overlay = img_resized.copy()\n",
        "    for r in regions:\n",
        "        y1, x1, y2, x2 = r.bbox\n",
        "        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
        "\n",
        "    # --- Step 8: Display ---\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"MRI Image\")\n",
        "    plt.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Segmentation Mask\")\n",
        "    plt.imshow(binary_mask, cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(f\"Detected Tumor Regions: {num_regions}\")\n",
        "    plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "BCnlatUFuXWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load model ---\n",
        "model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded successfully.\")\n",
        "\n",
        "# --- Preprocessing transform (same as training) ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Load a test image ---\n",
        "img_path = \"/content/drive/MyDrive/test_images/sample-img7-notumor.jpg\"   # ðŸ‘ˆ change to your image path\n",
        "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "img_resized = cv2.resize(img, (224, 224))\n",
        "\n",
        "# --- Prepare tensor ---\n",
        "input_tensor = transform(img_resized).unsqueeze(0).to(device)  # [1,1,224,224]\n",
        "\n",
        "# --- Predict mask ---\n",
        "with torch.no_grad():\n",
        "    pred_mask = torch.sigmoid(model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "# --- Normalize prediction ---\n",
        "pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "\n",
        "# --- Remove noise: set values below 0.1 to 0 ---\n",
        "noise_threshold = 0.1\n",
        "pred_mask[pred_mask < noise_threshold] = 0\n",
        "\n",
        "# --- Create heatmap ---\n",
        "heatmap = cv2.applyColorMap(np.uint8(255 * pred_mask), cv2.COLORMAP_JET)\n",
        "\n",
        "# --- Overlay heatmap on original grayscale image ---\n",
        "overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "# --- Display results ---\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img_resized, cmap='gray')\n",
        "plt.title(\"Original MRI\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(pred_mask, cmap='jet')\n",
        "plt.title(\"Predicted Mask (Noise Removed)\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Heatmap Overlay (Cleaned)\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A9J6-Fz9z0lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load classification model (ViT) ---\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "\n",
        "cls_model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "cls_model = ViTForImageClassification.from_pretrained(cls_model_path)\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(cls_model_path)\n",
        "cls_model.to(device)\n",
        "cls_model.eval()\n",
        "\n",
        "# --- Load segmentation model (UNet) ---\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "cls_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "])\n",
        "\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Function to process input ---\n",
        "def classify_and_heatmap(img):\n",
        "    # Convert to PIL if needed\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img_pil = img\n",
        "\n",
        "    # --- Classification ---\n",
        "    inputs = cls_transform(img_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = cls_model(inputs).logits\n",
        "        pred_idx = outputs.argmax(dim=1).item()\n",
        "\n",
        "    classes = ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
        "    pred_class = classes[pred_idx]\n",
        "\n",
        "    # --- Initialize outputs ---\n",
        "    heatmap_overlay = None\n",
        "\n",
        "    # --- Generate heatmap if tumor exists ---\n",
        "    if pred_class != 'no_tumor':\n",
        "        # Preprocess grayscale for UNet\n",
        "        img_gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)\n",
        "        img_resized = cv2.resize(img_gray, (224,224))\n",
        "        input_tensor = seg_transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_mask = torch.sigmoid(seg_model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "        # Normalize & remove noise\n",
        "        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "        pred_mask[pred_mask < 0.1] = 0\n",
        "\n",
        "        # Create heatmap overlay\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * pred_mask), cv2.COLORMAP_JET)\n",
        "        heatmap_overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.6, heatmap, 0.4, 0)\n",
        "        heatmap_overlay = cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return pred_class, heatmap_overlay\n",
        "\n",
        "# --- Gradio interface ---\n",
        "iface = gr.Interface(\n",
        "    fn=classify_and_heatmap,\n",
        "    inputs=gr.Image(type=\"numpy\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Predicted Class\"),\n",
        "        gr.Image(label=\"Tumor Heatmap Overlay\")\n",
        "    ],\n",
        "    title=\"Brain Tumor Classification + Heatmap\",\n",
        "    description=\"Upload an MRI image: classifies tumor type and generates heatmap if tumor exists.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "30Wt9WEy2HuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load classification model ---\n",
        "cls_model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "cls_processor = ViTImageProcessor.from_pretrained(cls_model_path)\n",
        "cls_model = ViTForImageClassification.from_pretrained(cls_model_path)\n",
        "cls_model.to(device)\n",
        "cls_model.eval()\n",
        "\n",
        "# --- Load segmentation model (UNet) ---\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Function ---\n",
        "def classify_and_heatmap(img):\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img_pil = img\n",
        "\n",
        "    # Classification\n",
        "    inputs = cls_processor(images=img_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = cls_model(inputs).logits\n",
        "        pred_idx = outputs.argmax(dim=1).item()\n",
        "\n",
        "    classes = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "    pred_class = classes[pred_idx]\n",
        "\n",
        "    # Initialize blank images\n",
        "    blank_img = np.zeros((224,224), dtype=np.uint8)\n",
        "    mask_img = blank_img.copy()\n",
        "    heatmap_overlay = cv2.cvtColor(blank_img.copy(), cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Generate heatmap if tumor exists\n",
        "    if pred_class != 'No Tumor':\n",
        "        img_gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)\n",
        "        img_resized = cv2.resize(img_gray, (224,224))\n",
        "        input_tensor = seg_transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_mask = torch.sigmoid(seg_model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "        pred_mask[pred_mask < 0.1] = 0\n",
        "\n",
        "        mask_img = (pred_mask * 255).astype(np.uint8)\n",
        "\n",
        "        heatmap = cv2.applyColorMap(mask_img, cv2.COLORMAP_JET)\n",
        "        heatmap_overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.6, heatmap, 0.4, 0)\n",
        "        heatmap_overlay = cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    class_text = f\"âœ… Predicted Class: {pred_class}\" if pred_class == 'No Tumor' else f\"âš ï¸ Predicted Class: {pred_class}\"\n",
        "\n",
        "    return class_text, mask_img, heatmap_overlay\n",
        "\n",
        "# --- Gradio Blocks UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ§  NeuraWorks: Brain Tumor Classifier & Heatmap Visualizer\")\n",
        "    gr.Markdown(\"Upload an MRI image to classify tumor type and generate a noise-free heatmap overlay.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inp = gr.Image(type=\"numpy\", label=\"Upload MRI Image\")\n",
        "        with gr.Column():\n",
        "            class_out = gr.Textbox(label=\"Classification Result\", interactive=False)\n",
        "            mask_out = gr.Image(label=\"Predicted Mask (Noise Removed)\")\n",
        "            heatmap_out = gr.Image(label=\"Heatmap Overlay\")\n",
        "\n",
        "    btn = gr.Button(\"Predict\")\n",
        "\n",
        "    btn.click(fn=classify_and_heatmap, inputs=inp, outputs=[class_out, mask_out, heatmap_out])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "g-jJvhaR3acA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load classification model (ViT) ---\n",
        "cls_model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "cls_processor = ViTImageProcessor.from_pretrained(cls_model_path)\n",
        "cls_model = ViTForImageClassification.from_pretrained(cls_model_path)\n",
        "cls_model.to(device)\n",
        "cls_model.eval()\n",
        "\n",
        "# --- Load segmentation model (UNet) ---\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Function for classification + heatmap ---\n",
        "def classify_and_heatmap(img):\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img_pil = img\n",
        "\n",
        "    # --- Classification ---\n",
        "    inputs = cls_processor(images=img_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = cls_model(inputs).logits\n",
        "        pred_idx = outputs.argmax(dim=1).item()\n",
        "\n",
        "    classes = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "    pred_class = classes[pred_idx]\n",
        "\n",
        "    # --- Initialize blank images ---\n",
        "    blank_img = np.zeros((224,224), dtype=np.uint8)\n",
        "    mask_img = blank_img.copy()\n",
        "    heatmap_overlay = cv2.cvtColor(blank_img.copy(), cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # --- Generate heatmap if tumor exists ---\n",
        "    if pred_class != 'No Tumor':\n",
        "        img_gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)\n",
        "        img_resized = cv2.resize(img_gray, (224,224))\n",
        "        input_tensor = seg_transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_mask = torch.sigmoid(seg_model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "        # Normalize & remove noise\n",
        "        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "        pred_mask[pred_mask < 0.1] = 0\n",
        "\n",
        "        mask_img = (pred_mask * 255).astype(np.uint8)\n",
        "\n",
        "        heatmap = cv2.applyColorMap(mask_img, cv2.COLORMAP_JET)\n",
        "        heatmap_overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.6, heatmap, 0.4, 0)\n",
        "        heatmap_overlay = cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # --- Color-coded class output using HTML ---\n",
        "    if pred_class == 'No Tumor':\n",
        "        class_text = f\"<p style='color:green; font-size:20px;'>âœ… Predicted Class: {pred_class}</p>\"\n",
        "    else:\n",
        "        class_text = f\"<p style='color:red; font-size:20px;'>âš ï¸ Predicted Class: {pred_class}</p>\"\n",
        "\n",
        "    return class_text, mask_img, heatmap_overlay\n",
        "\n",
        "# --- Gradio Blocks UI with custom CSS ---\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    body {background-color: #f0f8ff;}\n",
        "    .gr-button {background-color: #1f77b4; color: white; font-size:16px;}\n",
        "    .gr-button:hover {background-color: #0059b3;}\n",
        "    .gr-textbox {font-size:18px;}\n",
        "\"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"<h2 style='color:#1f77b4;'>ðŸ§  NeuraWorks: Brain Tumor Classifier & Heatmap Visualizer</h2>\")\n",
        "    gr.Markdown(\"<p>Upload an MRI image to classify tumor type and generate a noise-free heatmap overlay.</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inp = gr.Image(type=\"numpy\", label=\"Upload MRI Image\")\n",
        "        with gr.Column():\n",
        "            class_out = gr.HTML(label=\"Classification Result\")\n",
        "            mask_out = gr.Image(label=\"Predicted Mask (Noise Removed)\")\n",
        "            heatmap_out = gr.Image(label=\"Heatmap Overlay\")\n",
        "\n",
        "    btn = gr.Button(\"Predict\")\n",
        "    btn.click(fn=classify_and_heatmap, inputs=inp, outputs=[class_out, mask_out, heatmap_out])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "leKH9L3h60m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load classification model ---\n",
        "cls_model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "cls_processor = ViTImageProcessor.from_pretrained(cls_model_path)\n",
        "cls_model = ViTForImageClassification.from_pretrained(cls_model_path)\n",
        "cls_model.to(device)\n",
        "cls_model.eval()\n",
        "\n",
        "# --- Load segmentation model ---\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Function for classification + heatmap ---\n",
        "def classify_and_heatmap(img):\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img_pil = img\n",
        "\n",
        "    # --- Classification ---\n",
        "    inputs = cls_processor(images=img_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = cls_model(inputs).logits\n",
        "        pred_idx = outputs.argmax(dim=1).item()\n",
        "\n",
        "    classes = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "    pred_class = classes[pred_idx]\n",
        "\n",
        "    # --- Initialize blank outputs ---\n",
        "    blank_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "    mask_img = blank_img.copy()\n",
        "    heatmap_overlay = blank_img.copy()\n",
        "\n",
        "    # --- Generate mask & heatmap if tumor exists ---\n",
        "    if pred_class != 'No Tumor':\n",
        "        img_gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)\n",
        "        img_resized = cv2.resize(img_gray, (224,224))\n",
        "        input_tensor = seg_transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_mask = torch.sigmoid(seg_model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "        # Normalize & remove noise\n",
        "        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "        pred_mask[pred_mask < 0.1] = 0\n",
        "\n",
        "        # --- Mask in bright red overlay ---\n",
        "        mask_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "        mask_img[:,:,0] = (pred_mask*255).astype(np.uint8)  # Red channel\n",
        "\n",
        "        # --- Heatmap overlay ---\n",
        "        heatmap = cv2.applyColorMap((pred_mask*255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "        heatmap_overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.5, heatmap, 0.5, 0)\n",
        "        heatmap_overlay = cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # --- Color-coded classification ---\n",
        "    if pred_class == 'No Tumor':\n",
        "        class_text = f\"<p style='color:green; font-size:22px;'>âœ… Predicted Class: {pred_class}</p>\"\n",
        "    else:\n",
        "        class_text = f\"<p style='color:red; font-size:22px;'>âš ï¸ Predicted Class: {pred_class}</p>\"\n",
        "\n",
        "    return class_text, mask_img, heatmap_overlay\n",
        "\n",
        "# --- Gradio Blocks UI with polished design ---\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    body {background-color: #f8f9fa;}\n",
        "    .gr-block {background-color: #ffffff; border-radius:10px; padding:15px;}\n",
        "    .gr-button {background-color:#007bff; color:white; font-size:16px; font-weight:bold;}\n",
        "    .gr-button:hover {background-color:#0056b3;}\n",
        "    .gr-html {font-size:20px; text-align:center;}\n",
        "\"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"<h1 style='text-align:center; color:#007bff;'>ðŸ§  NeuraWorks: Brain Tumor Classifier & Heatmap</h1>\")\n",
        "    gr.Markdown(\"<p style='text-align:center;'>Upload an MRI image to predict tumor type and generate a clean, noise-free heatmap overlay.</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Input column\n",
        "        with gr.Column(scale=1):\n",
        "            inp = gr.Image(type=\"numpy\", label=\"Upload MRI Image\")\n",
        "            btn = gr.Button(\"Predict\", variant=\"primary\")\n",
        "\n",
        "        # Output column\n",
        "        with gr.Column(scale=1):\n",
        "            class_out = gr.HTML(label=\"Classification Result\")\n",
        "            mask_out = gr.Image(label=\"Tumor Mask (Red Overlay)\")\n",
        "            heatmap_out = gr.Image(label=\"Heatmap Overlay\")\n",
        "\n",
        "    # Button click event\n",
        "    btn.click(fn=classify_and_heatmap, inputs=inp, outputs=[class_out, mask_out, heatmap_out])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "UlVO_wsB77mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load classification model ---\n",
        "cls_model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "cls_processor = ViTImageProcessor.from_pretrained(cls_model_path)\n",
        "cls_model = ViTForImageClassification.from_pretrained(cls_model_path)\n",
        "cls_model.to(device)\n",
        "cls_model.eval()\n",
        "\n",
        "# --- Load segmentation model ---\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Function for classification + heatmap ---\n",
        "def classify_and_heatmap(img):\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img_pil = img\n",
        "\n",
        "    # --- Classification ---\n",
        "    inputs = cls_processor(images=img_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = cls_model(inputs).logits\n",
        "        pred_idx = outputs.argmax(dim=1).item()\n",
        "\n",
        "    classes = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "    pred_class = classes[pred_idx]\n",
        "\n",
        "    # --- Initialize blank outputs ---\n",
        "    blank_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "    mask_img = blank_img.copy()\n",
        "    heatmap_overlay = blank_img.copy()\n",
        "\n",
        "    # --- Generate mask & heatmap if tumor exists ---\n",
        "    if pred_class != 'No Tumor':\n",
        "        img_gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)\n",
        "        img_resized = cv2.resize(img_gray, (224,224))\n",
        "        input_tensor = seg_transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_mask = torch.sigmoid(seg_model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "        # Normalize & remove noise\n",
        "        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "        pred_mask[pred_mask < 0.1] = 0\n",
        "\n",
        "        # --- Mask in bright red overlay ---\n",
        "        mask_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "        mask_img[:,:,0] = (pred_mask*255).astype(np.uint8)  # Red channel\n",
        "\n",
        "        # --- Heatmap overlay ---\n",
        "        heatmap = cv2.applyColorMap((pred_mask*255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "        heatmap_overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.5, heatmap, 0.5, 0)\n",
        "        heatmap_overlay = cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # --- Color-coded classification ---\n",
        "    if pred_class == 'No Tumor':\n",
        "        class_text = f\"<p style='color:green; font-size:22px;'>âœ… Predicted Class: {pred_class}</p>\"\n",
        "    else:\n",
        "        class_text = f\"<p style='color:red; font-size:22px;'>âš ï¸ Predicted Class: {pred_class}</p>\"\n",
        "\n",
        "    return class_text, img_pil, mask_img, heatmap_overlay\n",
        "\n",
        "# --- Function to save mask & heatmap for download ---\n",
        "def classify_with_download(img):\n",
        "    class_text, orig_img, mask_img, heatmap_img = classify_and_heatmap(img)\n",
        "    # Save files\n",
        "    mask_file = \"mask.png\"\n",
        "    heatmap_file = \"heatmap.png\"\n",
        "    cv2.imwrite(mask_file, cv2.cvtColor(mask_img, cv2.COLOR_RGB2BGR))\n",
        "    cv2.imwrite(heatmap_file, cv2.cvtColor(heatmap_img, cv2.COLOR_RGB2BGR))\n",
        "    return class_text, orig_img, mask_img, heatmap_img, mask_file, heatmap_file\n",
        "\n",
        "# --- Gradio Blocks UI ---\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    body {background-color: #f5f5f5;}\n",
        "    .gr-block {background-color: #ffffff; border-radius:10px; padding:15px;}\n",
        "    .gr-button {background-color:#007bff; color:white; font-size:16px; font-weight:bold;}\n",
        "    .gr-button:hover {background-color:#0056b3;}\n",
        "    .gr-html {font-size:20px; text-align:center;}\n",
        "    .gr-image {border: 1px solid #ddd; border-radius: 8px;}\n",
        "\"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"<h1 style='text-align:center; color:#007bff;'>ðŸ§  NeuraWorks: Brain Tumor Classifier & Heatmap</h1>\")\n",
        "    gr.Markdown(\"<p style='text-align:center;'>Upload an MRI image to predict tumor type and generate a clean, noise-free mask and heatmap overlay.</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Input column\n",
        "        with gr.Column(scale=1):\n",
        "            inp = gr.Image(type=\"numpy\", label=\"Upload MRI Image\")\n",
        "            btn = gr.Button(\"Predict\", variant=\"primary\")\n",
        "\n",
        "        # Output column\n",
        "        with gr.Column(scale=1):\n",
        "            class_out = gr.HTML(label=\"Classification Result\")\n",
        "\n",
        "            with gr.Tabs():\n",
        "                with gr.Tab(\"Original MRI\"):\n",
        "                    orig_out = gr.Image(label=\"Original MRI\", interactive=False)\n",
        "                with gr.Tab(\"Tumor Mask\"):\n",
        "                    mask_out = gr.Image(label=\"Tumor Mask (Red Overlay)\", interactive=False)\n",
        "                    mask_dl = gr.File(label=\"Download Mask\")\n",
        "                with gr.Tab(\"Heatmap Overlay\"):\n",
        "                    heatmap_out = gr.Image(label=\"Heatmap Overlay\", interactive=False)\n",
        "                    heatmap_dl = gr.File(label=\"Download Heatmap\")\n",
        "\n",
        "    # --- Button click event ---\n",
        "    btn.click(\n",
        "        fn=classify_with_download,\n",
        "        inputs=inp,\n",
        "        outputs=[class_out, orig_out, mask_out, heatmap_out, mask_dl, heatmap_dl]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "beWpqOG89MCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load classification model ---\n",
        "cls_model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "cls_processor = ViTImageProcessor.from_pretrained(cls_model_path)\n",
        "cls_model = ViTForImageClassification.from_pretrained(cls_model_path)\n",
        "cls_model.to(device)\n",
        "cls_model.eval()\n",
        "\n",
        "# --- Load segmentation model ---\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Classification + Heatmap function ---\n",
        "def classify_and_heatmap(img):\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img_pil = img\n",
        "\n",
        "    # --- Classification ---\n",
        "    inputs = cls_processor(images=img_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = cls_model(inputs).logits\n",
        "        pred_idx = outputs.argmax(dim=1).item()\n",
        "\n",
        "    classes = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "    pred_class = classes[pred_idx]\n",
        "\n",
        "    # --- Initialize blank outputs ---\n",
        "    blank_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "    mask_img = blank_img.copy()\n",
        "    heatmap_overlay = blank_img.copy()\n",
        "\n",
        "    # --- Generate mask & heatmap if tumor exists ---\n",
        "    if pred_class != 'No Tumor':\n",
        "        img_gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)\n",
        "        img_resized = cv2.resize(img_gray, (224,224))\n",
        "        input_tensor = seg_transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_mask = torch.sigmoid(seg_model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "        # Normalize & remove noise\n",
        "        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "        pred_mask[pred_mask < 0.1] = 0\n",
        "\n",
        "        # --- Mask in bright red overlay ---\n",
        "        mask_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "        mask_img[:,:,0] = (pred_mask*255).astype(np.uint8)  # Red channel\n",
        "\n",
        "        # --- Heatmap overlay ---\n",
        "        heatmap = cv2.applyColorMap((pred_mask*255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "        heatmap_overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.5, heatmap, 0.5, 0)\n",
        "        heatmap_overlay = cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # --- Color-coded classification ---\n",
        "    if pred_class == 'No Tumor':\n",
        "        class_text = f\"<p style='color:green; font-size:22px;'>âœ… Predicted Class: {pred_class}</p>\"\n",
        "    else:\n",
        "        class_text = f\"<p style='color:red; font-size:22px;'>âš ï¸ Predicted Class: {pred_class}</p>\"\n",
        "\n",
        "    return class_text, img_pil, mask_img, heatmap_overlay\n",
        "\n",
        "# --- Function for download files ---\n",
        "def classify_with_download(img):\n",
        "    class_text, orig_img, mask_img, heatmap_img = classify_and_heatmap(img)\n",
        "    # Save files\n",
        "    mask_file = \"mask.png\"\n",
        "    heatmap_file = \"heatmap.png\"\n",
        "    cv2.imwrite(mask_file, cv2.cvtColor(mask_img, cv2.COLOR_RGB2BGR))\n",
        "    cv2.imwrite(heatmap_file, cv2.cvtColor(heatmap_img, cv2.COLOR_RGB2BGR))\n",
        "    return class_text, orig_img, mask_img, heatmap_img, mask_file, heatmap_file\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    body {background-color: #f0f2f5;}\n",
        "    .gr-block {background-color: #ffffff; border-radius:12px; padding:20px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);}\n",
        "    .gr-button {background-color:#007bff; color:white; font-size:16px; font-weight:bold;}\n",
        "    .gr-button:hover {background-color:#0056b3;}\n",
        "    .gr-html {font-size:20px; text-align:center;}\n",
        "    .gr-image {border: 1px solid #ddd; border-radius: 8px;}\n",
        "    .info-card {background-color:#e9ecef; border-radius:8px; padding:10px; margin-top:10px;}\n",
        "\"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"<h1 style='text-align:center; color:#007bff;'>ðŸ§  NeuraWorks: Brain Tumor Classifier & Heatmap</h1>\")\n",
        "    gr.Markdown(\"<p style='text-align:center;'>Upload an MRI image to predict tumor type and generate a clear, noise-free mask and heatmap overlay.</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Input column\n",
        "        with gr.Column(scale=1):\n",
        "            inp = gr.Image(type=\"numpy\", label=\"Upload MRI Image\")\n",
        "            btn = gr.Button(\"Predict\", variant=\"primary\")\n",
        "\n",
        "        # Output column\n",
        "        with gr.Column(scale=1):\n",
        "            class_out = gr.HTML(label=\"Classification Result\")\n",
        "\n",
        "            # Info card\n",
        "            info_text = \"\"\"\n",
        "            <div class=\"info-card\">\n",
        "            <h4>How to interpret:</h4>\n",
        "            <ul>\n",
        "                <li><b>Original MRI:</b> The uploaded MRI scan.</li>\n",
        "                <li><b>Tumor Mask:</b> Red regions indicate tumor area.</li>\n",
        "                <li><b>Heatmap Overlay:</b> Shows tumor intensity & localization.</li>\n",
        "                <li><b>Classification:</b> Green = No Tumor, Red = Tumor detected.</li>\n",
        "            </ul>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            gr.HTML(info_text)\n",
        "\n",
        "            # Tabs for outputs\n",
        "            with gr.Tabs():\n",
        "                with gr.Tab(\"Original MRI\"):\n",
        "                    orig_out = gr.Image(label=\"Original MRI\", interactive=False)\n",
        "                with gr.Tab(\"Tumor Mask\"):\n",
        "                    mask_out = gr.Image(label=\"Tumor Mask (Red Overlay)\", interactive=False)\n",
        "                    mask_dl = gr.File(label=\"Download Mask\")\n",
        "                with gr.Tab(\"Heatmap Overlay\"):\n",
        "                    heatmap_out = gr.Image(label=\"Heatmap Overlay\", interactive=False)\n",
        "                    heatmap_dl = gr.File(label=\"Download Heatmap\")\n",
        "\n",
        "    # Button click event\n",
        "    btn.click(\n",
        "        fn=classify_with_download,\n",
        "        inputs=inp,\n",
        "        outputs=[class_out, orig_out, mask_out, heatmap_out, mask_dl, heatmap_dl]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "h4bZM64T-Hff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load classification model ---\n",
        "cls_model_path = \"/content/drive/MyDrive/brain_tumor_vit_model\"\n",
        "cls_processor = ViTImageProcessor.from_pretrained(cls_model_path)\n",
        "cls_model = ViTForImageClassification.from_pretrained(cls_model_path)\n",
        "cls_model.to(device)\n",
        "cls_model.eval()\n",
        "\n",
        "# --- Load segmentation model ---\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# --- Classification + Heatmap ---\n",
        "def classify_and_heatmap(img):\n",
        "    if isinstance(img, np.ndarray):\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img_pil = img\n",
        "\n",
        "    # --- Classification ---\n",
        "    inputs = cls_processor(images=img_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = cls_model(inputs).logits\n",
        "        pred_idx = outputs.argmax(dim=1).item()\n",
        "\n",
        "    classes = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "    pred_class = classes[pred_idx]\n",
        "\n",
        "    # --- Initialize blank outputs ---\n",
        "    blank_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "    mask_img = blank_img.copy()\n",
        "    heatmap_overlay = blank_img.copy()\n",
        "\n",
        "    # --- Generate mask & heatmap if tumor exists ---\n",
        "    if pred_class != 'No Tumor':\n",
        "        img_gray = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2GRAY)\n",
        "        img_resized = cv2.resize(img_gray, (224,224))\n",
        "        input_tensor = seg_transform(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_mask = torch.sigmoid(seg_model(input_tensor))[0,0].cpu().numpy()\n",
        "\n",
        "        # Normalize & remove noise\n",
        "        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())\n",
        "        pred_mask[pred_mask < 0.1] = 0\n",
        "\n",
        "        # --- Mask in bright red overlay ---\n",
        "        mask_img = np.zeros((224,224,3), dtype=np.uint8)\n",
        "        mask_img[:,:,0] = (pred_mask*255).astype(np.uint8)\n",
        "\n",
        "        # --- Heatmap overlay ---\n",
        "        heatmap = cv2.applyColorMap((pred_mask*255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "        heatmap_overlay = cv2.addWeighted(cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR), 0.5, heatmap, 0.5, 0)\n",
        "        heatmap_overlay = cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # --- Color-coded classification ---\n",
        "    if pred_class == 'No Tumor':\n",
        "        class_text = f\"<p style='color:green; font-size:22px;'>âœ… Predicted Class: {pred_class}</p>\"\n",
        "    else:\n",
        "        class_text = f\"<p style='color:red; font-size:22px;'>âš ï¸ Predicted Class: {pred_class}</p>\"\n",
        "\n",
        "    return class_text, img_pil, mask_img, heatmap_overlay\n",
        "\n",
        "# --- Function for download files ---\n",
        "def classify_with_download(img):\n",
        "    class_text, orig_img, mask_img, heatmap_img = classify_and_heatmap(img)\n",
        "    # Save files\n",
        "    mask_file = \"mask.png\"\n",
        "    heatmap_file = \"heatmap.png\"\n",
        "    cv2.imwrite(mask_file, cv2.cvtColor(mask_img, cv2.COLOR_RGB2BGR))\n",
        "    cv2.imwrite(heatmap_file, cv2.cvtColor(heatmap_img, cv2.COLOR_RGB2BGR))\n",
        "    return class_text, orig_img, mask_img, heatmap_img, mask_file, heatmap_file\n",
        "\n",
        "# --- Gradio UI with loading spinner ---\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    body {background-color: #f0f2f5;}\n",
        "    .gr-block {background-color: #ffffff; border-radius:12px; padding:20px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);}\n",
        "    .gr-button {background-color:#007bff; color:white; font-size:16px; font-weight:bold;}\n",
        "    .gr-button:hover {background-color:#0056b3;}\n",
        "    .gr-html {font-size:20px; text-align:center;}\n",
        "    .gr-image {border: 1px solid #ddd; border-radius: 8px;}\n",
        "    .info-card {background-color:#e9ecef; border-radius:8px; padding:10px; margin-top:10px;}\n",
        "    .spinner {border: 6px solid #f3f3f3; border-top: 6px solid #007bff; border-radius: 50%; width: 40px; height: 40px; animation: spin 1s linear infinite; margin:auto;}\n",
        "    @keyframes spin {0% { transform: rotate(0deg);} 100% { transform: rotate(360deg);}}\n",
        "\"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"<h1 style='text-align:center; color:#007bff;'>ðŸ§  NeuraWorks: Brain Tumor Classifier & Heatmap</h1>\")\n",
        "    gr.Markdown(\"<p style='text-align:center;'>Upload an MRI image to predict tumor type and generate a clear, noise-free mask and heatmap overlay.</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            inp = gr.Image(type=\"numpy\", label=\"Upload MRI Image\")\n",
        "            btn = gr.Button(\"Predict\", variant=\"primary\")\n",
        "            spinner = gr.HTML('<div class=\"spinner\" style=\"display:none;\"></div>')\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            class_out = gr.HTML(label=\"Classification Result\")\n",
        "            gr.HTML(\"\"\"\n",
        "            <div class=\"info-card\">\n",
        "                <h4>How to interpret:</h4>\n",
        "                <ul>\n",
        "                    <li><b>Original MRI:</b> The uploaded MRI scan.</li>\n",
        "                    <li><b>Tumor Mask:</b> Red regions indicate tumor area.</li>\n",
        "                    <li><b>Heatmap Overlay:</b> Shows tumor intensity & localization.</li>\n",
        "                    <li><b>Classification:</b> Green = No Tumor, Red = Tumor detected.</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Tabs():\n",
        "                with gr.Tab(\"Original MRI\"):\n",
        "                    orig_out = gr.Image(label=\"Original MRI\", interactive=False)\n",
        "                with gr.Tab(\"Tumor Mask\"):\n",
        "                    mask_out = gr.Image(label=\"Tumor Mask (Red Overlay)\", interactive=False)\n",
        "                    mask_dl = gr.File(label=\"Download Mask\")\n",
        "                with gr.Tab(\"Heatmap Overlay\"):\n",
        "                    heatmap_out = gr.Image(label=\"Heatmap Overlay\", interactive=False)\n",
        "                    heatmap_dl = gr.File(label=\"Download Heatmap\")\n",
        "\n",
        "    # --- JS to show/hide spinner ---\n",
        "    def show_spinner():\n",
        "        return '<div class=\"spinner\"></div>'\n",
        "\n",
        "    # --- Click event with spinner ---\n",
        "    btn.click(\n",
        "        fn=classify_with_download,\n",
        "        inputs=inp,\n",
        "        outputs=[class_out, orig_out, mask_out, heatmap_out, mask_dl, heatmap_dl]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "v2fVBoDY-_RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# At the top of your script\n",
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_trained_model.pth\", map_location=device))\n",
        "seg_model.eval()\n"
      ],
      "metadata": {
        "id": "ZMzNGsjk_iPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(seg_model.state_dict(), \"/content/drive/MyDrive/brats_weights.pth\")\n"
      ],
      "metadata": {
        "id": "yfNOP4Vx_l9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seg_model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "seg_model.load_state_dict(torch.load(\"/content/drive/MyDrive/brats_weights.pth\", map_location=device))\n",
        "seg_model.eval()\n"
      ],
      "metadata": {
        "id": "P0nDs_zuAKZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rl24J3w24sDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}